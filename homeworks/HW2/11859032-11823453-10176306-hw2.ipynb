{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n",
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import functools\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412, 92160, 3, 192, 0, 363, 34, 1441, 0, 174134, 0, 200, 0, 894, 412, 2652, 0, 810, 107, 49, 4903, 420, 0, 1, 48, 35, 489, 0, 35, 687, 192, 243, 0, 249311, 1877, 0, 1651, 1174, 0, 2701, 117, 412, 0, 810, 391, 245233, 1225, 5838, 16, 0, 233156, 3496, 0, 393, 17, 0, 2435, 4819, 930, 0, 0, 200, 0, 894, 0, 22, 398, 145, 0, 3, 271, 115, 0, 1176, 2777, 292, 0, 725, 192, 0, 0, 50046, 0, 1901, 1130, 0, 192, 0, 408, 0, 243779, 0, 0, 553, 192, 0, 363, 0, 3747, 0, 0, 0, 0, 1176, 0, 1239, 0, 0, 1115, 17, 0, 0, 585, 192, 1963, 0, 0, 412, 54356, 0, 773, 0, 0, 0, 192, 0, 0, 1130, 0, 363, 0, 545, 192, 0, 1174, 1901, 1130, 0, 4, 398, 145, 39, 0, 577, 0, 355, 0, 491, 0, 6025, 0, 0, 193156, 88, 34, 437, 0, 0, 1852, 0, 828, 0, 1588, 0, 0, 0, 2615, 0, 0, 107, 49, 420, 0, 0, 190, 7, 714, 2701, 0, 237, 192, 157, 0, 412, 34, 437, 0, 0, 200, 6025, 26, 0, 0, 0, 0, 363, 0, 22, 398, 145, 0, 200, 638, 126222, 6018, 0, 880, 0, 0, 161, 0, 0, 319, 894, 2701, 0, 0, 0, 301, 1200, 0, 363, 251, 430, 0, 207, 0, 76143, 1773, 0, 243779, 0, 0, 72030, 0, 55, 4903, 420, 0, 2701, 1496, 420, 0, 25480, 0, 420, 0, 0, 200, 0, 392, 2949, 0, 1738, 0, 61, 0, 71, 79, 0, 200, 903, 0, 188, 53, 6, 0, 476, 2, 0, 2028, 97, 334, 0, 0, 200, 178, 0, 0, 107, 49, 0, 214, 0, 0, 0, 114, 3866, 1505, 195, 79893, 574, 0, 198, 2160, 0, 192, 0, 420, 0, 384, 0, 2701, 0, 114, 6025, 1549, 74627, 0, 238, 0, 0, 0, 3729, 0, 192, 0, 79893, 0, 0, 729, 3141, 129, 0, 192, 196764, 39, 0, 0, 714, 63, 0, 55, 420, 3356, 0, 0, 117, 412, 0, 0, 79758, 0, 1901, 1130, 4067, 2133, 0, 0, 875, 72, 0, 0, 336, 2789, 0, 0, 25, 920, 121, 104, 0, 3162, 0, 0, 420, 0, 2178, 0, 0, 386, 192545, 159306, 0, 0, 0, 1914, 0, 200, 0, 1794, 0, 2654, 0, 0, 25480, 420, 0, 2795, 0, 0, 229690, 0, 32559, 0, 0, 392, 253919, 0, 0, 0, 0, 379, 0, 0, 114, 0, 553, 10, 0, 1128, 0, 23610, 248, 151, 0, 418, 0, 651, 0, 36, 0, 0, 645, 0, 0, 513, 0, 0, 25480, 420, 34, 0, 0, 0, 15, 0, 3348, 0, 3496, 0, 35, 687, 0, 1, 48, 0, 0, 2803, 0, 0, 714, 1274, 0, 114, 62, 1006, 70268, 1200, 2357, 0, 497, 0, 497, 125, 0, 913, 4647, 3985, 0, 0, 3370, 245233, 0, 0, 687, 0, 4, 1288, 0, 0, 0, 0, 715, 0, 0, 687, 583, 0, 0, 1627, 0, 0, 11, 357, 1359, 0, 849, 0, 0, 1518, 462, 245233, 0, 0, 0, 0, 0, 0, 171, 70268, 0))\n"
     ]
    }
   ],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time')]\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 29.279672384262085 seconds.\n"
     ]
    }
   ],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "# the total number of documents\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "# the pyindri dictionary\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "# tokenize the queries\n",
    "# query_id --> [token_id1, token_id2, ...] \n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "# gather all of the query token_ids into a set TODO why? \n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "#### Inverted index creation. \n",
    "# per document dictionaries\n",
    "document_lengths = {} # doc_id -> length of document\n",
    "unique_terms_per_document = {} # doc_id -> number of unique tokens in document\n",
    "\n",
    "# (query_term, (doc_id)) -> document TF\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "# (query_term_id) -> frequency of occurence of query_term_id in all of the documents \n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "# total number of words in the document\n",
    "total_terms = 0\n",
    "\n",
    "start_time = time.time()\n",
    "doc_id_to_ext_id = {}\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    doc_id_to_ext_id[int_doc_id] = ext_doc_id\n",
    "     \n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    \n",
    "    \n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "    \n",
    "    # accumulate TF and CF\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "    \n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    #if os.path.exists(run_out_path):\n",
    "    #    return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    \n",
    "    \n",
    "    score_sums = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    \n",
    "    for query_id, query_terms in queries.items():\n",
    "        for query_term_id in tokenized_queries[query_id]:\n",
    "            # TODO what happens when the query term doesn't occur in any document\n",
    "            for doc_id, document_term_freq in inverted_index[query_term_id].items():\n",
    "                score = score_fn(doc_id, query_term_id, document_term_freq)\n",
    "                score_sums[query_id][doc_id] += score\n",
    "                \n",
    "    data = {}\n",
    "    for query_id, document_scores in score_sums.items():\n",
    "        doc_scores = []\n",
    "        for doc_id in document_scores.keys():\n",
    "            doc_scores.append((document_scores[doc_id], doc_id_to_ext_id[doc_id]))\n",
    "        doc_scores.sort(key = lambda _: -_[0])\n",
    "        data[query_id] = doc_scores\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(int_document_id, query_term_id, document_term_freq):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\"\n",
    "    idf = math.log(num_documents / len(inverted_index[query_term_id]))\n",
    "    tf = math.log(1 + document_term_freq)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(int_document_id, query_term_id, document_term_freq, k_1 = 1.2, b = 0.75):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    idf = math.log(num_documents / len(inverted_index[query_term_id]))\n",
    "    tf_numerator = (k_1 + 1) * document_term_freq \n",
    "    tf_denom = k_1 * ((1 - b) + b * (document_lengths[int_document_id] / avg_doc_length)) + document_term_freq\n",
    "    return idf * (tf_numerator / tf_denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jelinek_mercer(int_document_id, query_term_id, document_term_freq, lmbd = 0.5):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    document_contrib = lmbd * (document_term_freq / document_lengths[int_document_id]) \n",
    "    corpus_contrib = (1 - lmbd) * (collection_frequencies[query_term_id] / total_terms)\n",
    "    return np.log(document_contrib + corpus_contrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_prior(int_document_id, query_term_id, document_term_freq, mu = 1000):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    #print(\"mu\", mu)\n",
    "    numerator = document_term_freq + mu * (collection_frequencies[query_term_id] / total_terms)\n",
    "    denom = document_lengths[int_document_id] + mu\n",
    "    return np.log(numerator / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_discounting(int_document_id, query_term_id, document_term_freq, delta = 0.1):\n",
    "    \"\"\"\n",
    "    sigma: discount constant [0,1]\n",
    "    delta: sigma*|du|/|d|\n",
    "    \"\"\"\n",
    "    sigma = (delta * unique_terms_per_document[int_document_id]) / document_lengths[int_document_id]\n",
    "    numerator = max(document_term_freq - delta, 0) \n",
    "    denominator = document_lengths[int_document_id]\n",
    "    discounted_prob = numerator / denominator\n",
    "    unigram_prob = collection_frequencies[query_term_id] / total_terms\n",
    "    return np.log(discounted_prob + (sigma * unigram_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #please check, by ece\n",
    "\n",
    "def gaussian_kernel(i, j, sigma):\n",
    "    return np.exp(-np.power((i-j),2)/(2*np.power(sigma,2)))\n",
    "\n",
    "def gaussian_kernel_estimation(i, j, sigma, N):\n",
    "    #TODO the one with the cumulative density function CDF\n",
    "    pass\n",
    "    \n",
    "def triangle_kernel(i, j, sigma): \n",
    "    absolute = np.absolute(i-j)\n",
    "    return 1 - (absolute/sigma) if(absolute <= sigma) else 0\n",
    "\n",
    "def cosine_kernel(i, j, sigma):\n",
    "    absolute = np.absolute(i-j) \n",
    "    k = 0.0\n",
    "    \n",
    "    if absolute <= sigma:\n",
    "        k = (1 + np.cos((absolute*math.pi)/sigma))/2\n",
    "    \n",
    "    return k\n",
    "\n",
    "def circle_kernel(i, j, sigma):\n",
    "    absolute = np.absolute(i-j) \n",
    "    k = 0.0\n",
    "    \n",
    "    if absolute <= sigma:\n",
    "        k = np.sqrt(1 - np.power(absolute/sigma, 2))\n",
    "    \n",
    "    return k\n",
    "\n",
    "def passage_kernel(i, j, sigma):\n",
    "    absolute = np.absolute(i-j)\n",
    "    return 1 if(absolute <= sigma) else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval_plm(model_name, score_fn):\n",
    "    \n",
    "    #please check, by ece\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    #if os.path.exists(run_out_path):\n",
    "    #    return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    \n",
    "    \n",
    "    score_sums = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    \n",
    "    plm_models_wdi = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(float)))\n",
    "            \n",
    "    scores_qdi = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(float)))\n",
    "                \n",
    "    \n",
    "    #MLE probs for query_lm[query_id][query_term_id]\n",
    "    query_lm = collections.defaultdict(lambda: collections.defaultdict(float)) \n",
    "    \n",
    "    for query_id, query_terms in queries.items():\n",
    "        query_word_counts = collections.defaultdict(float)\n",
    "        tokenized_qs = tokenized_queries[query_id]\n",
    "        for query_term_id in tokenized_qs:\n",
    "            query_word_counts[query_term_id] += 1 #P(w_i|q)\n",
    "         \n",
    "        query_word_counts.update((k,v/len(tokenized_qs)) for k,v in query_word_counts.items())#this was buggy, changed it\n",
    "       \n",
    "        query_lm[query_id] = query_word_counts #for each query, keep LM = P(w|q) for each unique word in the query\n",
    "    \n",
    "    doc_tokens_with_stop = {}\n",
    "    \n",
    "    # extract tokens without stop words\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        doc_tokens_with_stop[int_doc_id] = [ind for ind in index.document(int_doc_id)[1]]\n",
    "    \n",
    "    for query_id, query_terms in queries.items():\n",
    "        for query_term_id in tokenized_queries[query_id]:\n",
    "            # TODO what happens when the query term doesn't occur in any document\n",
    "            for doc_id, document_term_freq in inverted_index[query_term_id].items():\n",
    "              \n",
    "                #positional LM\n",
    "                plm_models_wdi[query_term_id][doc_id] = score_fn(doc_id, query_term_id, document_term_freq,\n",
    "                                                                 doc_tokens_with_stop[doc_id])\n",
    "                q_lm = query_lm[query_id][query_term_id]\n",
    "                \n",
    "                full_doc_length = len(doc_tokens_with_stop[doc_id])\n",
    "                \n",
    "                for i in range(full_doc_length):\n",
    "                    #for one word\n",
    "                    \n",
    "                    #final scores given query, doc and index\n",
    "                    scores_qdi[query_id][doc_id][i] -= q_lm * np.log(q_lm / plm_models_wdi[query_term_id][doc_id][i]) \n",
    "                    #print(i,scores_qdi[query_id][doc_id][i])\n",
    "               \n",
    "                #scores_sum SHOULD BE RETURNED as the scores of each doc\n",
    "                #print(max(list(scores_qdi[query_id][doc_id].values())))\n",
    "                \n",
    "                #oprint(score_sums[query_id][doc_id])\n",
    "                score_sums[query_id][doc_id] = max(list(scores_qdi[query_id][doc_id].values()))\n",
    "                #print(query_id, doc_id,doc_id_to_ext_id[doc_id], score_sums[query_id][doc_id] )\n",
    "                \n",
    "    data = {}\n",
    "    for query_id, document_scores in score_sums.items():\n",
    "        doc_scores = []\n",
    "        for doc_id in document_scores.keys():\n",
    "            doc_scores.append((document_scores[doc_id], doc_id_to_ext_id[doc_id]))\n",
    "        doc_scores.sort(key = lambda _: -_[0])\n",
    "        data[query_id] = doc_scores\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLM(int_document_id, query_term_id, document_term_freq, indices, kernel = gaussian_kernel, sigma = 50, mu = 1500):\n",
    "\n",
    "    #please check, by ece\n",
    "    \n",
    "    #change kernel?\n",
    "    #change mu according to optimized dirichlet hyperparameter!\n",
    "    #sigma is given in the assignment\n",
    "    \n",
    "    #query_term_id = w\n",
    "    #c_prime is for w,i\n",
    "    \n",
    "    full_doc_length = len(indices)\n",
    "    \n",
    "    c_prime = np.zeros(full_doc_length)\n",
    "    \n",
    "    #get the positional word ids in the document, eliminate stop words where ind==0\n",
    "    #TODO i and j after removing 0 or are we keeping the indices from the actual sequence?\n",
    "    #WE ARE KEEPING EVERYTHING IN THE DOCUMENT\n",
    "     \n",
    "    \n",
    "    #denominator, virtual document lengths\n",
    "    z = np.zeros(full_doc_length)\n",
    "    \n",
    "    for i in range(full_doc_length):\n",
    "        \n",
    "        #kernel outcomes for the given i over all j, they will be summed up for z[i]\n",
    "        k = np.zeros(full_doc_length)\n",
    "        \n",
    "        for j in range(full_doc_length):\n",
    "            \n",
    "            cwj = 0.0\n",
    "            if indices[j] == query_term_id:\n",
    "                cwj = 1.0\n",
    "            \n",
    "            k[j] = kernel(i,j, sigma) #TODO ij or ji\n",
    "            c_prime[i] += cwj*k[j]\n",
    "            \n",
    "        z[i] = np.sum(k)   #sum over kernel(j,i) j=1 to N, not the final simplified version\n",
    "        #NOT USING the sum of c_prime over all w, but using zi as the sum over kernels\n",
    "        \n",
    "    unigram_prob = collection_frequencies[query_term_id] / total_terms\n",
    "    \n",
    "    pm_wdi = (c_prime + mu * unigram_prob)/(z + mu)\n",
    "    \n",
    "    #language model per word given document and a position\n",
    "    return pm_wdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using PLM\n",
      "51 39937 AP891002-0137 -3.32710682308\n",
      "51 108551 AP880915-0047 -3.31976389652\n",
      "51 141472 AP880706-0025 -3.06194641513\n",
      "51 76813 AP891017-0091 -3.3326605372\n",
      "51 96270 AP880805-0037 -3.07922249955\n",
      "51 143375 AP881222-0148 -3.33263931017\n",
      "51 107011 AP880820-0096 -3.06398468725\n",
      "51 131092 AP880804-0167 -2.64648918325\n",
      "51 114710 AP880906-0086 -3.3208541051\n",
      "51 25623 AP890906-0311 -3.32554842467\n",
      "51 54298 AP890810-0244 -3.33270464798\n",
      "51 116772 AP880312-0117 -3.33265750279\n",
      "51 100389 AP881020-0170 -3.32398635083\n",
      "51 138278 AP881231-0042 -3.32983504572\n",
      "51 141660 AP880706-0213 -3.31768121141\n",
      "51 77866 AP890131-0301 -3.33270454178\n",
      "51 95275 AP880311-0301 -2.97707556526\n",
      "51 31789 AP890609-0270 -3.33174437555\n",
      "51 93533 AP881230-0079 -3.33269870393\n",
      "51 19516 AP890517-0089 -3.14001462084\n",
      "51 78911 AP891219-0213 -3.33270464616\n",
      "51 65602 AP890125-0094 -3.32249208381\n",
      "51 20548 AP890216-0288 -2.78288290298\n",
      "51 76470 AP890608-0253 -2.72581865045\n",
      "51 81990 AP890110-0304 -3.00101634377\n",
      "51 17480 AP890614-0287 -3.32826898324\n",
      "51 25271 AP890426-0260 -2.9367204259\n",
      "51 120909 AP880815-0175 -3.32937629128\n",
      "51 10320 AP891211-0325 -3.31959531842\n",
      "51 67665 AP890117-0252 -3.33239705805\n",
      "51 135011 AP881203-0143 -3.32869099087\n",
      "51 55382 AP890209-0234 -2.95232734115\n",
      "51 78692 AP890821-0246 -3.31900954918\n",
      "51 70747 AP890708-0097 -2.95221103657\n",
      "51 82012 AP890520-0011 -3.33265087377\n",
      "51 142191 AP881119-0077 -3.32289212818\n",
      "51 64613 AP890412-0286 -3.32572736895\n",
      "51 18540 AP890323-0197 -3.3325961616\n",
      "51 138164 AP880803-0232 -3.33270464867\n",
      "51 38001 AP890731-0272 -3.2186835998\n",
      "51 94324 AP880316-0292 -2.6227009111\n",
      "51 80063 AP890623-0269 -3.33270444946\n",
      "51 16234 AP890816-0147 -3.33270286988\n",
      "51 73854 AP891128-0199 -3.33262032034\n",
      "51 109695 AP881108-0253 -2.51592546019\n",
      "51 62593 AP890511-0222 -3.33270464798\n",
      "51 8324 AP890208-0146 -3.33260914821\n",
      "51 11398 AP891118-0182 -3.31423058048\n",
      "51 47245 AP890720-0240 -3.33270174525\n",
      "51 34958 AP891116-0258 -3.33270444946\n",
      "51 89232 AP881202-0163 -3.32973567058\n",
      "51 5269 AP891101-0286 -3.32745093163\n",
      "51 33943 AP890920-0131 -3.33269918618\n",
      "51 35524 AP891011-0255 -2.74910987219\n",
      "51 61595 AP890620-0248 -2.92373497734\n",
      "51 86172 AP880707-0038 -3.32745777127\n",
      "51 37024 AP890206-0170 -3.31518373231\n",
      "51 88228 AP881229-0171 -3.33270462717\n",
      "51 18599 AP890323-0256 -3.33270255111\n",
      "51 141480 AP880706-0033 -2.98985871043\n",
      "51 98475 AP880713-0028 -2.97713651494\n",
      "51 98479 AP880713-0032 -3.33151759953\n",
      "51 17779 AP890719-0295 -3.31644406804\n",
      "51 39454 AP890413-0243 -3.33270464867\n",
      "51 63672 AP890228-0177 -3.3225039593\n",
      "51 86201 AP880707-0067 -3.33246427073\n",
      "51 113852 AP881130-0023 -3.33270464246\n",
      "51 38077 AP890301-0076 -3.32445167651\n",
      "51 6335 AP891006-0222 -3.33245835588\n",
      "51 76249 AP890608-0032 -3.33270442333\n",
      "51 40134 AP891002-0334 -2.98374107352\n",
      "51 40139 AP891002-0339 -3.31423058048\n",
      "51 86223 AP880707-0089 -2.91141978618\n",
      "51 130424 AP880627-0115 -2.82529570928\n",
      "51 128377 AP880928-0210 -3.17690170962\n",
      "51 123407 AP880709-0147 -3.31320530986\n",
      "51 35034 AP890703-0072 -3.31022475858\n",
      "51 44251 AP890322-0331 -3.33269722387\n",
      "51 141534 AP880706-0087 -3.15517860558\n",
      "51 97234 AP880525-0347 -3.32596186012\n",
      "51 37091 AP890206-0237 -3.33167018519\n",
      "51 29926 AP890306-0244 -3.33061653946\n",
      "51 24808 AP890111-0078 -3.33245835588\n",
      "51 97513 AP880329-0252 -3.31486748992\n",
      "51 39147 AP890419-0224 -3.33270464823\n",
      "51 36076 AP890105-0233 -3.0934784661\n",
      "51 39150 AP890419-0227 -3.32144755264\n",
      "51 29935 AP890306-0253 -3.3327044978\n",
      "51 141552 AP880706-0105 -3.3136304736\n",
      "51 51444 AP890928-0294 -3.31917687566\n",
      "51 76021 AP890914-0250 -3.0088771248\n",
      "51 80118 AP890724-0045 -3.32973566324\n",
      "51 86263 AP880707-0129 -3.33082492235\n",
      "51 29946 AP890306-0264 -3.33269115118\n",
      "51 251 AP890425-0251 -3.33270464865\n",
      "51 48380 AP890804-0141 -3.332704641\n",
      "51 8446 AP890208-0268 -3.33140292066\n",
      "51 105729 AP880704-0009 -3.33261491942\n",
      "51 34058 AP890920-0246 -3.32328629591\n",
      "51 98572 AP880713-0125 -3.33270436076\n",
      "51 98573 AP880713-0126 -3.17998941888\n",
      "51 105743 AP880704-0023 -2.78914952001\n",
      "51 105745 AP880704-0025 -3.33054277999\n",
      "51 35096 AP890703-0134 -3.31393731533\n",
      "51 84250 AP890518-0275 -3.33269722386\n",
      "51 86300 AP880707-0166 -3.33270464867\n",
      "51 116004 AP880901-0073 -3.08841487178\n",
      "51 105766 AP880704-0046 -3.33108993373\n",
      "51 111153 AP880513-0252 -3.06631296983\n",
      "51 105770 AP880704-0050 -3.32575718014\n",
      "51 130348 AP880627-0039 -2.7984003273\n",
      "51 104245 AP880929-0250 -3.33270455461\n",
      "51 105778 AP880704-0058 -3.33268907286\n",
      "51 105783 AP880704-0063 -3.32779351772\n",
      "51 105785 AP880704-0065 -3.33270464867\n",
      "51 94527 AP880726-0137 -3.32984841402\n",
      "51 20801 AP890626-0236 -3.3327046479\n",
      "51 91019 AP880714-0170 -3.33079370551\n",
      "51 130372 AP880627-0063 -3.00617456688\n",
      "51 105798 AP880704-0078 -3.33270464867\n",
      "51 11593 AP890707-0114 -3.06141578401\n",
      "51 132427 AP880712-0034 -3.33270271669\n",
      "51 105805 AP880704-0085 -3.3326966393\n",
      "51 105809 AP880704-0089 -3.21085312412\n",
      "51 134558 AP880516-0328 -3.33166495661\n",
      "51 95571 AP880721-0269 -3.33258976484\n",
      "51 141655 AP880706-0208 -3.32232634463\n",
      "51 140632 AP880822-0141 -3.33227998796\n",
      "51 105820 AP880704-0100 -3.3327046485\n",
      "51 105821 AP880704-0101 -3.33230184939\n",
      "51 64865 AP891207-0246 -3.33270464604\n",
      "51 130402 AP880627-0093 -2.60758928279\n",
      "51 8549 AP890305-0051 -3.33270464866\n",
      "51 95592 AP880721-0290 -2.84898154405\n",
      "51 124988 AP881021-0256 -3.32089599835\n",
      "51 68972 AP890723-0009 -3.32533704497\n",
      "51 63853 AP890222-0111 -3.31731119328\n",
      "51 105839 AP880704-0119 -3.32655166221\n",
      "51 15592 AP891127-0256 -3.00921949086\n",
      "51 105843 AP880704-0123 -3.06631460705\n",
      "51 16617 AP890106-0255 -2.97583994392\n",
      "51 17784 AP890719-0300 -3.33206481943\n",
      "51 98681 AP880713-0234 -3.33270464867\n",
      "51 93563 AP881230-0109 -3.33270464848\n",
      "51 105854 AP880704-0134 -3.33270463704\n",
      "51 105856 AP880704-0136 -3.04876225517\n",
      "51 92548 AP880808-0072 -3.32779333429\n",
      "51 105861 AP880704-0141 -3.33269533902\n",
      "51 105864 AP880704-0144 -3.06631460705\n",
      "51 105866 AP880704-0146 -3.33026870641\n",
      "51 105867 AP880704-0147 -3.01270642951\n",
      "51 141708 AP880706-0261 -3.31518484645\n",
      "51 105872 AP880704-0152 -3.33162142894\n",
      "51 41880 AP890901-0154 -3.3303055504\n",
      "51 5523 AP890613-0247 -3.33220799159\n",
      "51 134548 AP880516-0318 -2.8389701334\n",
      "51 123285 AP880709-0025 -3.33146336953\n",
      "51 141758 AP880706-0311 -2.63058038308\n",
      "51 66968 AP891003-0130 -3.13485910425\n",
      "51 39151 AP890419-0228 -2.95740049004\n",
      "51 2460 AP890211-0169 -3.33270461653\n",
      "51 75165 AP891221-0274 -2.9833920814\n",
      "51 81310 AP890427-0238 -3.33270464758\n",
      "51 69024 AP890723-0061 -3.32995816073\n",
      "51 80624 AP890629-0275 -2.7708105876\n",
      "51 13730 AP890929-0096 -3.02154112046\n",
      "51 41371 AP890526-0222 -3.33270464867\n",
      "51 117156 AP880318-0287 -2.5346683312\n",
      "51 123303 AP880709-0043 -3.33270464257\n",
      "51 79273 AP890406-0027 -3.33054417113\n",
      "51 114091 AP881130-0262 -3.01376858242\n",
      "51 35544 AP891011-0275 -3.31768121141\n",
      "51 54702 AP891121-0110 -3.332704596\n",
      "51 33181 AP890407-0321 -3.33257494212\n",
      "51 119217 AP881002-0062 -3.33269016062\n",
      "51 123314 AP880709-0054 -3.3297356698\n",
      "51 130484 AP880627-0175 -2.98270764768\n",
      "51 58805 AP890803-0196 -3.33215451193\n",
      "51 94646 AP880626-0003 -2.72575840897\n",
      "51 141556 AP880706-0109 -3.33230184922\n",
      "51 64323 AP890612-0290 -3.00243600689\n",
      "51 10683 AP890509-0269 -3.33269301232\n",
      "51 1470 AP890112-0243 -3.33270366159\n",
      "51 86261 AP880707-0127 -3.00558546369\n",
      "51 17856 AP890224-0066 -3.33263871945\n",
      "51 132546 AP880712-0153 -3.11834675593\n",
      "51 92235 AP880905-0132 -3.00769626434\n",
      "51 107972 AP881129-0057 -3.32555265084\n",
      "51 123333 AP880709-0073 -3.33269601197\n",
      "51 94662 AP880626-0019 -3.31644364053\n",
      "51 123338 AP880709-0078 -3.33269601197\n",
      "51 24011 AP890622-0084 -3.32740396851\n",
      "51 80332 AP890724-0259 -3.31537487562\n",
      "51 120568 AP881227-0145 -3.33270438631\n",
      "51 96722 AP880908-0136 -3.31828920182\n",
      "51 62883 AP891209-0045 -3.32674021156\n",
      "51 123348 AP880709-0088 -3.31320530986\n",
      "51 124373 AP880309-0405 -3.33101958571\n",
      "51 132566 AP880712-0173 -3.33199829879\n",
      "51 94681 AP880626-0038 -2.58416359409\n",
      "51 110044 AP880717-0007 -2.83708871127\n",
      "51 92638 AP880808-0162 -3.32729928561\n",
      "51 119265 AP880710-0012 -3.32144755264\n",
      "51 81378 AP890427-0306 -3.09073889997\n",
      "51 108005 AP881129-0090 -3.33270255111\n",
      "51 9702 AP890503-0279 -2.61180566716\n",
      "51 55377 AP890209-0229 -3.3170658669\n",
      "51 123368 AP880709-0108 -3.16721314363\n",
      "51 123369 AP880709-0109 -3.01545536334\n",
      "51 116202 AP880901-0271 -2.9245100365\n",
      "51 116204 AP880901-0273 -3.33212558498\n",
      "51 52818 AP890107-0160 -3.33234264522\n",
      "51 115187 AP880806-0016 -3.33264724531\n",
      "51 141691 AP880706-0244 -3.13666999464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 119295 AP880710-0042 -3.33171718467\n",
      "51 16896 AP890911-0278 -3.32483573755\n",
      "51 53763 AP891222-0290 -3.33270433405\n",
      "51 107013 AP880820-0098 -3.31344627063\n",
      "51 132616 AP880712-0223 -3.33134737178\n",
      "51 46607 AP890411-0191 -3.32950000879\n",
      "51 136511 AP880325-0293 -2.64654716315\n",
      "51 82450 AP891103-0231 -3.33134774951\n",
      "51 75284 AP890103-0117 -3.33270337528\n",
      "51 71192 AP891117-0207 -3.33269776836\n",
      "51 134685 AP880411-0118 -3.32653898375\n",
      "51 71198 AP891117-0213 -2.77120536482\n",
      "51 71199 AP891117-0214 -2.95334996638\n",
      "51 123427 AP880709-0167 -3.31359158128\n",
      "51 119333 AP880710-0080 -3.33269601197\n",
      "51 81330 AP890427-0258 -3.33270464866\n",
      "51 123438 AP880709-0178 -3.32950000879\n",
      "51 39473 AP890413-0262 -3.33198215336\n",
      "51 122419 AP880827-0035 -3.33269533902\n",
      "51 130613 AP880627-0304 -3.32950000879\n",
      "51 141661 AP880706-0214 -3.33196288347\n",
      "51 96823 AP880908-0237 -3.33270464867\n",
      "51 126521 AP881019-0150 -3.32871128642\n",
      "51 72252 AP890104-0259 -2.73326489278\n",
      "51 91712 AP880229-0271 -3.33261973214\n",
      "51 112195 AP880628-0087 -3.31859016827\n",
      "51 49734 AP890606-0198 -2.99612997036\n",
      "51 112200 AP880628-0092 -3.31859016827\n",
      "51 44618 AP890328-0202 -2.89086997071\n",
      "51 112205 AP880628-0097 -2.89585945215\n",
      "51 104924 AP880923-0136 -3.33251961692\n",
      "51 111185 AP880513-0284 -3.08789863808\n",
      "51 9810 AP890823-0072 -3.33218189622\n",
      "51 115290 AP880526-0021 -3.32871128667\n",
      "51 85263 AP880503-0244 -2.83692837359\n",
      "51 123322 AP880709-0062 -3.33270464257\n",
      "51 22112 AP890908-0122 -3.3055389815\n",
      "51 20070 AP890127-0117 -3.33218189622\n",
      "51 47718 AP890217-0158 -2.99777883618\n",
      "51 73323 AP890207-0230 -2.83081779209\n",
      "51 9837 AP890823-0099 -3.33221840877\n",
      "51 136821 AP880715-0151 -3.33270464867\n",
      "51 73333 AP890207-0240 -3.33266606805\n",
      "51 97911 AP880719-0051 -3.33115784879\n",
      "51 109179 AP880812-0142 -3.33167023302\n",
      "51 73344 AP890207-0251 -3.33268907286\n",
      "51 141493 AP880706-0046 -3.32837726916\n",
      "51 57992 AP890601-0339 -3.32050813863\n",
      "51 84076 AP890518-0101 -3.33157071943\n",
      "51 24947 AP890111-0217 -3.33026870641\n",
      "51 20108 AP890127-0155 -3.33209601714\n",
      "51 117389 AP880215-0180 -3.33245092742\n",
      "51 2703 AP890621-0238 -3.33269461742\n",
      "51 129681 AP880720-0294 -3.01635772342\n",
      "51 79509 AP890406-0263 -3.31359158127\n",
      "51 112278 AP880628-0170 -2.83598450317\n",
      "51 141707 AP880706-0260 -3.33006486885\n",
      "51 132761 AP880718-0071 -3.33252849686\n",
      "51 73370 AP890207-0277 -3.33255269252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cd41ff78f895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#run_retrieval('dirichlet_prior', dirichlet_prior)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#run_retrieval('absolute_discounting', absolute_discounting)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrun_retrieval_plm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PLM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPLM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# TODO implement the rest of the retrieval functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-8f5b0f87e7ef>\u001b[0m in \u001b[0;36mrun_retrieval_plm\u001b[0;34m(model_name, score_fn)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m#positional LM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 plm_models_wdi[query_term_id][doc_id] = score_fn(doc_id, query_term_id, document_term_freq,\n\u001b[0;32m---> 54\u001b[0;31m                                                                  doc_tokens_with_stop[doc_id])\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mq_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_lm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-25752e6b6624>\u001b[0m in \u001b[0;36mPLM\u001b[0;34m(int_document_id, query_term_id, document_term_freq, indices, kernel, sigma, mu)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO ij or ji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mc_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcwj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#sum over kernel(j,i) j=1 to N, not the final simplified version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# combining the two functions above: \n",
    "#run_retrieval('tfidf', tfidf)\n",
    "#run_retrieval('bm25', bm25)\n",
    "#run_retrieval('jelinek_mercer', jelinek_mercer)\n",
    "#run_retrieval('dirichlet_prior', dirichlet_prior)\n",
    "#run_retrieval('absolute_discounting', absolute_discounting)\n",
    "run_retrieval_plm('PLM', PLM)\n",
    "# TODO implement the rest of the retrieval functions \n",
    "\n",
    "# TODO implement tools to help you with the analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P_5': 0.2133,\n",
       " 'map_cut_1000': 0.173,\n",
       " 'ndcg_cut_10': 0.2382,\n",
       " 'recall_1000': 0.5728}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TrecAPI:\n",
    "    def __init__(self, trec_path = \"../../trec_eval/trec_eval\"):\n",
    "        self.trec_path = os.path.abspath(trec_path)\n",
    "    \n",
    "    def evaluate(self, test_file_name, prediction_file_name, metrics_to_capture=None, granular = False):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        if metrics_to_capture is None:\n",
    "            metrics_to_capture = {\"ndcg_cut_10\", \"map_cut_1000\", \"P_5\", \"recall_1000\"}\n",
    "        try:\n",
    "            command = [self.trec_path, \"-m\", \"all_trec\", \"-q\", test_file_name, prediction_file_name]\n",
    "            output = subprocess.check_output(command, universal_newlines=True)\n",
    "            data = collections.defaultdict(dict)\n",
    "            for line in output.split(\"\\n\"):\n",
    "                # ignore empty lines\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                    \n",
    "                metric, query, value = line.split(\"\\t\")\n",
    "                \n",
    "                if not granular and query != \"all\":\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                metric = metric.strip()\n",
    "                    \n",
    "                # ignore metrics we don't care about\n",
    "                if metric not in metrics_to_capture:\n",
    "                    continue\n",
    "                \n",
    "                # relstring is a binary string, don't convert to float\n",
    "                if metric not in {\"relstring\"}:\n",
    "                    value = float(value)\n",
    "                \n",
    "                data[query][metric] = value \n",
    "            \n",
    "            if not granular:\n",
    "                return data[\"all\"]\n",
    "            \n",
    "            return data\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(e.output)\n",
    "            return None\n",
    "        \n",
    "\n",
    "trec = TrecAPI()\n",
    "results = trec.evaluate(\"./ap_88_89/qrel_validation\", \"./dirichlet_prior.run\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_eval(grid, score_fn, test_file, verbose = False, output_folder = \"grid_results/\"):\n",
    "    \"\"\"\n",
    "    Evaluate a scoring method on a grid of possible values.\n",
    "    grid : a dictionary of `param -> list of possible values`\n",
    "    scoring_method: the scoring method to evaluate\n",
    "    returns TODO\n",
    "    \"\"\"\n",
    "    # convert to an list because indexing is required\n",
    "    grid = list(grid.items())\n",
    "    param_lengths = tuple([len(grid_item[1]) for grid_item in grid])\n",
    "    print(\"Running grid eval on {} possible combinations\".format(np.prod(param_lengths)))\n",
    "    \n",
    "    all_indices = np.ndindex(param_lengths)\n",
    "    grid_results = []\n",
    "    for index, values in enumerate(all_indices):\n",
    "        param_grid = {}\n",
    "        for param_index, param_val_index in enumerate(values):\n",
    "            param_name, param_values = grid[param_index]\n",
    "            param_grid[param_name] = param_values[param_val_index]\n",
    "        if verbose:\n",
    "            print(\"Running evaluation {} of {}, with params: {}\".format(index + 1, np.prod(param_lengths), param_grid))\n",
    "        output_path = os.path.join(output_folder, \"{}_{}\".format(score_fn.__name__, index))\n",
    "        # construct the scoring function\n",
    "        part_score_fn = functools.partial(score_fn, **param_grid) \n",
    "        run_retrieval(output_path, part_score_fn)\n",
    "        metrics = trec.evaluate(test_file, output_path + \".run\")\n",
    "        grid_results.append((param_grid, metrics))\n",
    "    return grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter tuning for JM\n",
    "Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.5, 0.9])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid eval on 10 possible combinations\n",
      "Running evaluation 1 of 10, with params: {'lmbd': 0.1}\n",
      "Retrieving using grid_results/jelinek_mercer_0\n",
      "Running evaluation 2 of 10, with params: {'lmbd': 0.18888888888888888}\n",
      "Retrieving using grid_results/jelinek_mercer_1\n",
      "Running evaluation 3 of 10, with params: {'lmbd': 0.2777777777777778}\n",
      "Retrieving using grid_results/jelinek_mercer_2\n",
      "Running evaluation 4 of 10, with params: {'lmbd': 0.3666666666666667}\n",
      "Retrieving using grid_results/jelinek_mercer_3\n",
      "Running evaluation 5 of 10, with params: {'lmbd': 0.4555555555555556}\n",
      "Retrieving using grid_results/jelinek_mercer_4\n",
      "Running evaluation 6 of 10, with params: {'lmbd': 0.5444444444444445}\n",
      "Retrieving using grid_results/jelinek_mercer_5\n",
      "Running evaluation 7 of 10, with params: {'lmbd': 0.6333333333333333}\n",
      "Retrieving using grid_results/jelinek_mercer_6\n",
      "Running evaluation 8 of 10, with params: {'lmbd': 0.7222222222222222}\n",
      "Retrieving using grid_results/jelinek_mercer_7\n",
      "Running evaluation 9 of 10, with params: {'lmbd': 0.8111111111111111}\n",
      "Retrieving using grid_results/jelinek_mercer_8\n",
      "Running evaluation 10 of 10, with params: {'lmbd': 0.9}\n",
      "Retrieving using grid_results/jelinek_mercer_9\n"
     ]
    }
   ],
   "source": [
    "grid_jm = {\n",
    "    \"lmbd\" : np.linspace(0.1, 0.9, 10)\n",
    "}\n",
    "jelinek_mercer_grid_results = grid_eval(grid_jm, jelinek_mercer, \"./ap_88_89/qrel_validation\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid(grid):\n",
    "    for params, results in grid:\n",
    "        print(\"Params: \", params)\n",
    "        print(\"\\tResults: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  {'lmbd': 0.1}\n",
      "\tResults:  {'map_cut_1000': 0.0789, 'ndcg_cut_10': 0.133, 'recall_1000': 0.2306, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.18888888888888888}\n",
      "\tResults:  {'map_cut_1000': 0.0794, 'ndcg_cut_10': 0.1331, 'recall_1000': 0.2344, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.2777777777777778}\n",
      "\tResults:  {'map_cut_1000': 0.0796, 'ndcg_cut_10': 0.1331, 'recall_1000': 0.2368, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.3666666666666667}\n",
      "\tResults:  {'map_cut_1000': 0.0798, 'ndcg_cut_10': 0.1331, 'recall_1000': 0.2384, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.4555555555555556}\n",
      "\tResults:  {'map_cut_1000': 0.0798, 'ndcg_cut_10': 0.1331, 'recall_1000': 0.2394, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.5444444444444445}\n",
      "\tResults:  {'map_cut_1000': 0.0799, 'ndcg_cut_10': 0.1332, 'recall_1000': 0.2395, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.6333333333333333}\n",
      "\tResults:  {'map_cut_1000': 0.0799, 'ndcg_cut_10': 0.1332, 'recall_1000': 0.2398, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.7222222222222222}\n",
      "\tResults:  {'map_cut_1000': 0.0799, 'ndcg_cut_10': 0.1332, 'recall_1000': 0.2398, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.8111111111111111}\n",
      "\tResults:  {'map_cut_1000': 0.08, 'ndcg_cut_10': 0.1332, 'recall_1000': 0.2405, 'P_5': 0.12}\n",
      "Params:  {'lmbd': 0.9}\n",
      "\tResults:  {'map_cut_1000': 0.08, 'ndcg_cut_10': 0.1332, 'recall_1000': 0.2405, 'P_5': 0.12}\n"
     ]
    }
   ],
   "source": [
    "print_grid(jelinek_mercer_grid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter tuning for Dirichlet Prior\n",
    "Dirichlet Prior (explore different values of 𝛍 [500, 1000, 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid eval on 7 possible combinations\n",
      "Running evaluation 1 of 7, with params: {'mu': 300}\n",
      "Retrieving using grid_results/dirichlet_prior_0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-df3575509707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"mu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1501\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m }\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdirichlet_grid_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_dirichlet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./ap_88_89/qrel_validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-3478f6371812>\u001b[0m in \u001b[0;36mgrid_eval\u001b[0;34m(grid, score_fn, test_file, verbose, output_folder)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# construct the scoring function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpart_score_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mrun_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart_score_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mgrid_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-411cf00dd90c>\u001b[0m in \u001b[0;36mrun_retrieval\u001b[0;34m(model_name, score_fn)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mout_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             max_objects_per_query=1000)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-af25bda75c74>\u001b[0m in \u001b[0;36mwrite_run\u001b[0;34m(model_name, data, out_f, max_objects_per_query, skip_sorting)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_sorting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mobject_assesments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_assesments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_objects_per_query\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_dirichlet = {\n",
    "    \"mu\": np.arange(300, 1501, 200)\n",
    "}\n",
    "dirichlet_grid_results = grid_eval(grid_dirichlet, dirichlet_prior, \"./ap_88_89/qrel_validation\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  {'mu': 500}\n",
      "\tResults:  {'map_cut_1000': 0.0838, 'ndcg_cut_10': 0.1467, 'recall_1000': 0.2513, 'P_5': 0.1333}\n",
      "Params:  {'mu': 700}\n",
      "\tResults:  {'map_cut_1000': 0.0831, 'ndcg_cut_10': 0.1438, 'recall_1000': 0.2492, 'P_5': 0.1333}\n",
      "Params:  {'mu': 900}\n",
      "\tResults:  {'map_cut_1000': 0.0827, 'ndcg_cut_10': 0.1389, 'recall_1000': 0.2483, 'P_5': 0.1267}\n",
      "Params:  {'mu': 1100}\n",
      "\tResults:  {'map_cut_1000': 0.0824, 'ndcg_cut_10': 0.1369, 'recall_1000': 0.2478, 'P_5': 0.1267}\n",
      "Params:  {'mu': 1300}\n",
      "\tResults:  {'map_cut_1000': 0.0822, 'ndcg_cut_10': 0.1371, 'recall_1000': 0.2477, 'P_5': 0.1333}\n",
      "Params:  {'mu': 1500}\n",
      "\tResults:  {'map_cut_1000': 0.0821, 'ndcg_cut_10': 0.1342, 'recall_1000': 0.2466, 'P_5': 0.1333}\n"
     ]
    }
   ],
   "source": [
    "print_grid(dirichlet_grid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter tuning for Absolute discounting\n",
    "Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.5, 0.9]). [5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid eval on 10 possible combinations\n",
      "Running evaluation 1 of 10, with params: {'delta': 0.1}\n",
      "Retrieving using grid_results/absolute_discounting_0\n",
      "Running evaluation 2 of 10, with params: {'delta': 0.18888888888888888}\n",
      "Retrieving using grid_results/absolute_discounting_1\n",
      "Running evaluation 3 of 10, with params: {'delta': 0.2777777777777778}\n",
      "Retrieving using grid_results/absolute_discounting_2\n",
      "Running evaluation 4 of 10, with params: {'delta': 0.3666666666666667}\n",
      "Retrieving using grid_results/absolute_discounting_3\n",
      "Running evaluation 5 of 10, with params: {'delta': 0.4555555555555556}\n",
      "Retrieving using grid_results/absolute_discounting_4\n",
      "Running evaluation 6 of 10, with params: {'delta': 0.5444444444444445}\n",
      "Retrieving using grid_results/absolute_discounting_5\n",
      "Running evaluation 7 of 10, with params: {'delta': 0.6333333333333333}\n",
      "Retrieving using grid_results/absolute_discounting_6\n",
      "Running evaluation 8 of 10, with params: {'delta': 0.7222222222222222}\n",
      "Retrieving using grid_results/absolute_discounting_7\n",
      "Running evaluation 9 of 10, with params: {'delta': 0.8111111111111111}\n",
      "Retrieving using grid_results/absolute_discounting_8\n",
      "Running evaluation 10 of 10, with params: {'delta': 0.9}\n",
      "Retrieving using grid_results/absolute_discounting_9\n"
     ]
    }
   ],
   "source": [
    "grid_abs_discounting = {\n",
    "    \"delta\": np.linspace(0.1, 0.9, 10)\n",
    "}\n",
    "abs_discounting_grid = grid_eval(grid_abs_discounting, absolute_discounting, \n",
    "                                 \"./ap_88_89/qrel_validation\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grid(abs_discounting_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toss_coin(heads = 0.5):\n",
    "    # head = True, Tails is False\n",
    "    return True if random.random() < heads else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomiser(list_a, list_b, N = 30):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    assert len(list_a) == len(list_b)\n",
    "    for i in range(N):\n",
    "        new_a, new_b = [], []\n",
    "        \n",
    "        for index in range(len(list_a)):\n",
    "            if toss_coin():\n",
    "                # don't swap, copy from source\n",
    "                new_a.append(list_a[index])\n",
    "                new_b.append(list_b[index])\n",
    "            else:\n",
    "                new_a.append(list_b[index])\n",
    "                new_b.append(list_a[index])\n",
    "        mean_new_a = np.mean(new_a)\n",
    "        mean_new_b = np.mean(new_b)\n",
    "        \n",
    "        yield mean_new_a, mean_new_b\n",
    "        \n",
    "a = np.transpose(list(randomiser([1,2,3],[4,5,6])))\n",
    "print(a)\n",
    "z, p = stats.ttest_rel(a[0],a[1])\n",
    "\n",
    "print(z,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bonferroni(groups, alpha = 0.05):\n",
    "    comparisons = math.factorial(groups)/(math.factorial(2)*math.factorial(groups - 2))\n",
    "    return alpha/comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_token_iterator():\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "        ll = [id2token[token_id].lower() for token_id in doc_token_ids if token_id > 0]\n",
    "        yield ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary, and saving...\n"
     ]
    }
   ],
   "source": [
    "def get_dictionary(dict_file_name = \"./index.dict\"):\n",
    "    \n",
    "    if os.path.exists(dict_file_name):\n",
    "        print(\"Loading existing dictionary\")\n",
    "        return gensim.corpora.Dictionary.load(dict_file_name)\n",
    "    print(\"Creating dictionary, and saving...\")\n",
    "    gensim_dict = gensim.corpora.Dictionary(document_token_iterator())\n",
    "    gensim_dict.save(dict_file_name)\n",
    "    \n",
    "    return gensim_dict\n",
    "gensim_dict = get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patons': 244141,\n",
       " 'demeaned': 91890,\n",
       " 'woundings': 71634,\n",
       " 'potasinskas': 133019,\n",
       " 'obiols': 247384,\n",
       " 'zacharias': 87110,\n",
       " 'conciliating': 204982,\n",
       " 'clasmates': 176447,\n",
       " 'lomaxs': 17225,\n",
       " 'salerno': 43352,\n",
       " 'boreanaz': 202023,\n",
       " 'gerontis': 139756,\n",
       " 'welcom': 153516,\n",
       " 'seimograph': 31253,\n",
       " 'platona': 232252,\n",
       " 'awardin': 108253,\n",
       " 'stray': 7610,\n",
       " 'vargas': 21367,\n",
       " 'stamatoiu': 164354,\n",
       " 'erleen': 207865,\n",
       " 'warbled': 180223,\n",
       " 'bibliotheque': 47301,\n",
       " 'jetprop': 97711,\n",
       " 'claassen': 122820,\n",
       " 'fisheries': 940,\n",
       " 'humberto': 54210,\n",
       " 'whisler': 210799,\n",
       " 'grohses': 261725,\n",
       " 'wrath': 16209,\n",
       " 'mcdlts': 85797,\n",
       " 'toiling': 27685,\n",
       " 'observance': 23835,\n",
       " 'yoshina': 208024,\n",
       " 'cowsll': 180113,\n",
       " 'adaption': 180794,\n",
       " 'ferdovzi': 255923,\n",
       " 'kanagawa': 112642,\n",
       " 'kherkel': 256516,\n",
       " 'pharmacists': 23516,\n",
       " 'wohlin': 160288,\n",
       " 'resurgency': 135919,\n",
       " '-5678': 86561,\n",
       " 'boson': 195620,\n",
       " 'februarys': 243,\n",
       " 'hitatement': 81637,\n",
       " '-1907': 151879,\n",
       " 'krasucki': 192779,\n",
       " 'willye': 123235,\n",
       " 'peggi': 247376,\n",
       " 'wojewodski': 248301,\n",
       " 'soles': 47643,\n",
       " 'earnned': 189530,\n",
       " 'giansanti': 96999,\n",
       " 'carrizosa': 195193,\n",
       " 'seldovia': 73401,\n",
       " 'palstinian': 58466,\n",
       " 'varity': 37873,\n",
       " '1983s': 197000,\n",
       " 'blind': 7045,\n",
       " 'muizuddin': 138060,\n",
       " 'protopapas': 195584,\n",
       " 'pesenti': 218450,\n",
       " 'elsijane': 76582,\n",
       " 'jiaying': 107555,\n",
       " 'matosinho': 224986,\n",
       " 'equivocated': 152301,\n",
       " 'sicker': 81159,\n",
       " 'turki': 146141,\n",
       " 'prosthesis': 55975,\n",
       " 'bispartisan': 128987,\n",
       " 'islandwide': 74514,\n",
       " 'turfan': 150004,\n",
       " 'estus': 44060,\n",
       " 'clemsons': 122321,\n",
       " 'caernarvon': 40494,\n",
       " '827s': 142159,\n",
       " 'catabola': 235000,\n",
       " 'prada': 207250,\n",
       " 'novovoronezhskiy': 118412,\n",
       " 'holds': 7062,\n",
       " 'tosaw': 219366,\n",
       " 'imry': 57301,\n",
       " 'carelses': 185815,\n",
       " 'teg1': 44535,\n",
       " 'antibody': 7041,\n",
       " 'brucella': 117437,\n",
       " 'gaibandha': 249741,\n",
       " '000336': 238170,\n",
       " 'agexporter': 67508,\n",
       " 'maxwelton': 218968,\n",
       " 'stuntmen': 147698,\n",
       " 'smiti': 130348,\n",
       " 'meaningul': 198899,\n",
       " 'loosiers': 106225,\n",
       " 'pariset': 61516,\n",
       " 'kishwaukee': 163282,\n",
       " 'neum': 141591,\n",
       " 'furlong': 116910,\n",
       " 'attire': 15688,\n",
       " 'pbgc': 47826,\n",
       " 'fizzing': 142430,\n",
       " 'rxa7': 118764,\n",
       " 'coodination': 174391,\n",
       " 'maike': 180044,\n",
       " 'wool': 18544,\n",
       " 'bisulfite': 165082,\n",
       " 'providential': 104182,\n",
       " 'arbs': 144970,\n",
       " 'lermonov': 211080,\n",
       " 'schlengers': 266853,\n",
       " 'scrum': 168365,\n",
       " 'damaj': 252712,\n",
       " 'geingob': 53623,\n",
       " 'ranchero': 8702,\n",
       " 'glowered': 41314,\n",
       " 'domestically': 13707,\n",
       " 'uruidez': 238789,\n",
       " 'kaiges': 211368,\n",
       " 'pan2': 113366,\n",
       " 'mohsen': 25327,\n",
       " 'kazi': 178755,\n",
       " 'fring': 247085,\n",
       " 'coughed': 19443,\n",
       " 'moviehouse': 177811,\n",
       " 'norges': 90776,\n",
       " 'konagai': 235096,\n",
       " 'sellersburg': 163293,\n",
       " 'overproducing': 113695,\n",
       " 'kalina': 83769,\n",
       " 'mlangeni': 78371,\n",
       " 'nws': 42984,\n",
       " '-280': 89001,\n",
       " 'pickoff': 249913,\n",
       " 'hark': 95748,\n",
       " 'ballengers': 260045,\n",
       " 'davars': 233078,\n",
       " 'phosphorescent': 97021,\n",
       " 'merow': 252057,\n",
       " 'qubeiba': 156496,\n",
       " 'microwatt': 252020,\n",
       " 'a18s': 111263,\n",
       " 'zinora': 85432,\n",
       " 'alexandrias': 53603,\n",
       " 'leitha': 140788,\n",
       " 'outfitting': 25211,\n",
       " 'philibert': 84204,\n",
       " 'overreach': 116955,\n",
       " 'treutinger': 220415,\n",
       " 'inadmissible': 43854,\n",
       " 'ngolo': 173235,\n",
       " 'bellarie': 74142,\n",
       " 'wentzville': 76242,\n",
       " 'antiterrorist': 137756,\n",
       " 'wa': 65013,\n",
       " 'impair': 34539,\n",
       " 'meanehile': 166067,\n",
       " 'nad': 44746,\n",
       " 'roofing': 22365,\n",
       " 'cavalrymen': 140543,\n",
       " 'skintight': 81593,\n",
       " 'transmissions': 13770,\n",
       " 'beaird': 55107,\n",
       " 'remembering': 19372,\n",
       " 'pinedale': 60607,\n",
       " 'megastudios': 241618,\n",
       " 'marich': 192975,\n",
       " 'abnormalites': 181221,\n",
       " 'wetc05': 148517,\n",
       " 'pelt': 48058,\n",
       " 'baltimoreans': 130753,\n",
       " 'lpls': 203289,\n",
       " 'enriches': 11303,\n",
       " 'whytes': 174005,\n",
       " 'buckles': 46928,\n",
       " 'povah': 238269,\n",
       " 'kasting': 237325,\n",
       " 'umw': 13074,\n",
       " 'b1313': 94423,\n",
       " 'helmet': 24854,\n",
       " 'schur': 124626,\n",
       " 'plug': 8815,\n",
       " 'raimona': 129271,\n",
       " 'macadamias': 158697,\n",
       " 'badreddin': 38749,\n",
       " 'kalmanovitch': 71318,\n",
       " 'bendezu': 178537,\n",
       " 'roommate': 36587,\n",
       " 'ragaee': 151392,\n",
       " 'pergonal': 144897,\n",
       " 'hamstring': 67923,\n",
       " 'arrogance': 21019,\n",
       " 'fling': 23020,\n",
       " 'entourage': 28328,\n",
       " 'murderess': 91132,\n",
       " 'podolsky': 14985,\n",
       " 'haquat': 258337,\n",
       " 'earful': 55420,\n",
       " 'malvinas': 48651,\n",
       " 'limo': 26055,\n",
       " 'yore': 56467,\n",
       " 'sleeve': 26932,\n",
       " 'eldad': 197679,\n",
       " 'roups': 83439,\n",
       " 'isssues': 187152,\n",
       " 'corrada': 197961,\n",
       " 'camerena': 66512,\n",
       " 'riccardi': 96813,\n",
       " 'agnositic': 107703,\n",
       " 'beaman': 98699,\n",
       " 'montagnard': 167938,\n",
       " 'logs': 8060,\n",
       " 'refugio': 68234,\n",
       " 'polygraphs': 109787,\n",
       " 'set': 163,\n",
       " 'heartand': 108862,\n",
       " 'genzer': 229390,\n",
       " 'burth': 243112,\n",
       " 'natioal': 110146,\n",
       " 'amcare': 153785,\n",
       " 'bajnok': 47546,\n",
       " 'ffb': 106284,\n",
       " 'bej19': 124226,\n",
       " 'ousmane': 192559,\n",
       " 'anotab': 79301,\n",
       " 'grandparernts': 162835,\n",
       " 'rizig': 103950,\n",
       " 'knicking': 221648,\n",
       " 'lorean': 74449,\n",
       " 'grigoriou': 238034,\n",
       " 'prefabricrated': 213330,\n",
       " 'subcontractor': 18110,\n",
       " 'servants': 7961,\n",
       " 'treasurer': 18060,\n",
       " 'casera': 66680,\n",
       " 'floren': 260969,\n",
       " 'whilso': 51752,\n",
       " 'misusing': 14918,\n",
       " 'chrysomelobia': 263442,\n",
       " 'weho': 256870,\n",
       " 'komsomol': 54165,\n",
       " '017': 36330,\n",
       " 'helfrick': 82799,\n",
       " 'vayadoot': 211586,\n",
       " 'duchesss': 132911,\n",
       " 'dillin': 239769,\n",
       " 'piromalli': 235526,\n",
       " '20095': 211799,\n",
       " '86175': 149240,\n",
       " 'rentells': 229616,\n",
       " 'shefaram': 231183,\n",
       " 'ideologicallly': 112458,\n",
       " 'follen': 187236,\n",
       " '9820': 163684,\n",
       " 'heseltine': 78387,\n",
       " 'meshki': 55523,\n",
       " 'wiggling': 8094,\n",
       " 'minke': 44510,\n",
       " 'subsoil': 13674,\n",
       " 'falschen': 182308,\n",
       " 'garlisi': 58647,\n",
       " 'kamifurano': 172423,\n",
       " '22s': 115617,\n",
       " 'uunica': 239159,\n",
       " 'outtasight': 152211,\n",
       " '4308': 222507,\n",
       " 'lambeth': 53155,\n",
       " 'catskillands': 264431,\n",
       " 'lys': 130804,\n",
       " 'dzidzava': 45701,\n",
       " 'brouwers': 149906,\n",
       " 'lockports': 186701,\n",
       " 'andersonstown': 143308,\n",
       " 'venal': 87165,\n",
       " 'ax12': 123679,\n",
       " 'extorts': 93625,\n",
       " 'premdas': 199686,\n",
       " 'lasmos': 252978,\n",
       " 'demarchi': 24348,\n",
       " 'micronase': 125452,\n",
       " 'kyrie': 105586,\n",
       " 'mcmunn': 265369,\n",
       " 'toucans': 201669,\n",
       " 'herassing': 190806,\n",
       " 'krzystof': 88705,\n",
       " 'doukas': 240288,\n",
       " 'stoppable': 234345,\n",
       " 'uncomfortable': 14644,\n",
       " 'gospodarcze': 225921,\n",
       " 'tuchman': 66557,\n",
       " 'anarchism': 69218,\n",
       " 'pedaled': 16923,\n",
       " 'rockwoods': 249354,\n",
       " 'boleslawski': 178528,\n",
       " 'elimate': 160729,\n",
       " 'newsworthy': 20964,\n",
       " 'intevention': 214310,\n",
       " 'cpo': 155292,\n",
       " 'affliates': 181752,\n",
       " 'solder': 99207,\n",
       " 'pogue': 62663,\n",
       " 'tebnins': 117649,\n",
       " 'duffeys': 146801,\n",
       " 'countrie': 225008,\n",
       " 'arniches': 134847,\n",
       " 'bentzs': 221160,\n",
       " 'zambezi': 85972,\n",
       " 'shvets': 223782,\n",
       " 'pavillion': 57940,\n",
       " 'matchpoints': 99487,\n",
       " 'audens': 48875,\n",
       " 'matronly': 78713,\n",
       " 'lavoe': 243575,\n",
       " 'speculation': 836,\n",
       " 'ek': 35934,\n",
       " 'rixe': 201273,\n",
       " 'kemenes': 50934,\n",
       " 'virgin': 10559,\n",
       " 'enterina': 78887,\n",
       " 'yanagiya': 245295,\n",
       " 'sinding': 81821,\n",
       " '023': 56260,\n",
       " 'fnns': 57810,\n",
       " 'overnment': 137645,\n",
       " 'gado': 204749,\n",
       " 'growl': 11954,\n",
       " 'frolicked': 60404,\n",
       " 'rabboos': 250763,\n",
       " 'bursatiles': 166047,\n",
       " 'supernovae': 106496,\n",
       " 'examiners': 14627,\n",
       " 'pw4000': 56289,\n",
       " 'concentraced': 236633,\n",
       " 'rodigari': 257032,\n",
       " 'cristiani': 16973,\n",
       " 'marauder': 150515,\n",
       " 'sailer': 72297,\n",
       " 'institite': 89090,\n",
       " 'nonfunctioning': 244149,\n",
       " 'temptations': 71950,\n",
       " 'committeemen': 168864,\n",
       " 'schallert': 103026,\n",
       " 'chickening': 211938,\n",
       " 'denouncing': 18294,\n",
       " 'crossbreds': 144985,\n",
       " 'perugia': 27985,\n",
       " 'ratifier': 224788,\n",
       " 'boaring': 221012,\n",
       " 'bevin': 42709,\n",
       " 'norgle': 55092,\n",
       " 'ghalis': 190310,\n",
       " 'soden': 55296,\n",
       " 'cocktail': 20830,\n",
       " 'batified': 194382,\n",
       " 'westview': 155341,\n",
       " 'stehems': 260395,\n",
       " 'affaire': 241517,\n",
       " 'martiny': 73013,\n",
       " 'coulam': 158518,\n",
       " 'chopers': 210111,\n",
       " 'operator': 3402,\n",
       " 'nonsummit': 136659,\n",
       " 'wynnewood': 143796,\n",
       " 'soka': 36191,\n",
       " 'glamuzina': 13557,\n",
       " 'caper': 44630,\n",
       " 'aleando': 257350,\n",
       " 'nondula': 203282,\n",
       " 'krumholz': 209478,\n",
       " 'jenk': 253763,\n",
       " 'sanctimonious': 67733,\n",
       " 'lawsy': 185874,\n",
       " 'wyborcza': 21618,\n",
       " 'mujahadeen': 38758,\n",
       " 'corina': 225834,\n",
       " 'hiroshimas': 107138,\n",
       " 'pitaluga': 245474,\n",
       " 'fdrs': 42419,\n",
       " 'b1051': 246571,\n",
       " 'deceptinets': 160048,\n",
       " 'ny17': 34969,\n",
       " 'reboiled': 115646,\n",
       " '-1076': 166196,\n",
       " 'wxft': 202717,\n",
       " 'boulogne': 60201,\n",
       " 'silberkleit': 160258,\n",
       " 'cheng': 40029,\n",
       " 'scheming': 11916,\n",
       " 'aircrft': 127839,\n",
       " 'kinzer': 197594,\n",
       " 'collpased': 88975,\n",
       " 'reaganist': 220611,\n",
       " 'deshutes': 196000,\n",
       " 'najah': 77316,\n",
       " 'sparano': 188256,\n",
       " 'kilgo': 136343,\n",
       " 'zaradic': 94979,\n",
       " 'amakye': 200437,\n",
       " 'hamir': 260532,\n",
       " 'claras': 47457,\n",
       " 'b0716': 172443,\n",
       " 'egotists': 164220,\n",
       " 'redefine': 12158,\n",
       " 'ostaro': 193363,\n",
       " 'maurianias': 111749,\n",
       " 'krizias': 130949,\n",
       " 'guanaco': 36859,\n",
       " 'gasoine': 80657,\n",
       " 'dewulf': 205134,\n",
       " 'semismograph': 60310,\n",
       " 'immunofluorescence': 157650,\n",
       " 'awsat': 28806,\n",
       " 'ebdcs': 75056,\n",
       " 'vented': 61662,\n",
       " 'roasters': 49995,\n",
       " 'bearded': 19071,\n",
       " 'eh': 73200,\n",
       " 'vela': 28558,\n",
       " 'savitchs': 202707,\n",
       " 'waterings': 193905,\n",
       " 'compile': 45354,\n",
       " 'maciunas': 247400,\n",
       " 'rediation': 105440,\n",
       " 'paladin': 55877,\n",
       " 'reportdly': 167275,\n",
       " 'mosts': 223132,\n",
       " 'xel': 135211,\n",
       " 'waleed': 221079,\n",
       " 'arnesen': 126182,\n",
       " 'indinga': 146310,\n",
       " 'dj060': 194453,\n",
       " 'tippah': 218077,\n",
       " 'vidman': 242850,\n",
       " 'relativistic': 198268,\n",
       " 'multisyllable': 222688,\n",
       " 'methylphenidate': 160227,\n",
       " 'dogrib': 226254,\n",
       " 'damgerous': 103710,\n",
       " 'vibrate': 76275,\n",
       " 'lapsed': 28808,\n",
       " 'mercantilism': 218625,\n",
       " 'appalls': 84179,\n",
       " 'pesnel': 231583,\n",
       " 'masahiko': 157668,\n",
       " 'b0694': 251729,\n",
       " '-810': 131182,\n",
       " 'ernie': 25267,\n",
       " '227': 24527,\n",
       " 'a0659': 111152,\n",
       " 'dan': 918,\n",
       " 'esrange': 148592,\n",
       " 'overbuilt': 97089,\n",
       " 'milners': 207170,\n",
       " 'malefactors': 61401,\n",
       " 'stridently': 86134,\n",
       " 'macit': 241895,\n",
       " 'gangly': 97746,\n",
       " 'tukarem': 154004,\n",
       " 'fowl': 26349,\n",
       " 'jetmaker': 118615,\n",
       " 'genoa': 52391,\n",
       " 'rheuban': 221766,\n",
       " 'leovigildo': 174611,\n",
       " 'michighan': 242741,\n",
       " 'diddly': 178671,\n",
       " 'cerebral': 16908,\n",
       " 'gunburst': 244389,\n",
       " 'impishly': 116130,\n",
       " 'fizzles': 23019,\n",
       " 'nobby': 205967,\n",
       " 'uk': 13502,\n",
       " 'oversleeping': 156862,\n",
       " 'cussler': 72259,\n",
       " '-1970': 5020,\n",
       " 'isepatesis': 264306,\n",
       " 'durakyan': 248977,\n",
       " 'condict': 67542,\n",
       " 'expository': 111684,\n",
       " 'peirce': 45633,\n",
       " 'mordernizing': 188132,\n",
       " 'schimel': 212732,\n",
       " 'bernknopf': 256072,\n",
       " 'treelayatewat': 228852,\n",
       " 'ichthyological': 156435,\n",
       " 'phealth': 233875,\n",
       " 'teaches': 13632,\n",
       " 'kowaleski': 179252,\n",
       " 'rich': 8074,\n",
       " 'commisssion': 64372,\n",
       " 'varanda': 160412,\n",
       " 'legislativ': 113468,\n",
       " 'texacos': 13902,\n",
       " 'sterilizer': 266583,\n",
       " 'unicycle': 87350,\n",
       " 'lysol': 143543,\n",
       " 'apostacy': 150036,\n",
       " 'quian': 180330,\n",
       " 'petrels': 97547,\n",
       " 'colonel': 5818,\n",
       " 'fossett': 157981,\n",
       " '23675': 223697,\n",
       " 'lucayan': 200651,\n",
       " 'machias': 83006,\n",
       " 'lakhtin': 126286,\n",
       " 'bankable': 49742,\n",
       " 'unanimous': 5453,\n",
       " 'nacelle': 184773,\n",
       " 'accecpted': 131898,\n",
       " 'feasibilty': 69791,\n",
       " 'classics': 21654,\n",
       " 'gompers': 111433,\n",
       " 'knasel': 216485,\n",
       " 'farges': 46841,\n",
       " 'gluckstern': 231585,\n",
       " 'scialfa': 162578,\n",
       " 'quoddy': 178162,\n",
       " 'pimp': 75370,\n",
       " 'sarsawi': 258021,\n",
       " 'dutilleux': 29778,\n",
       " 'peackeeping': 19238,\n",
       " 'karelians': 84320,\n",
       " 'zahran': 215147,\n",
       " 'hogan': 17095,\n",
       " 'wamcs': 95596,\n",
       " 'hamiltons': 42229,\n",
       " 'kinyani': 63793,\n",
       " 'ales': 141250,\n",
       " 'glorioso': 211290,\n",
       " 'noah': 48006,\n",
       " 'herders': 9330,\n",
       " 'shehu': 72697,\n",
       " 'philhamonic': 209105,\n",
       " 'hosilities': 201522,\n",
       " 'opena': 213749,\n",
       " 'camshafts': 254004,\n",
       " 'bersteins': 154787,\n",
       " 'cholim': 130580,\n",
       " 'tiresome': 50719,\n",
       " 'mossel': 69968,\n",
       " 'secours': 252788,\n",
       " 'wagnalls': 256178,\n",
       " 'inanimate': 42181,\n",
       " 'bengina': 252542,\n",
       " 'kevorkov': 196235,\n",
       " 'unamuno': 220941,\n",
       " 'mestizos': 112961,\n",
       " 'beachers': 101476,\n",
       " 'ya': 34322,\n",
       " 'bleeds': 93304,\n",
       " 'kleber': 41230,\n",
       " 'estimations': 140868,\n",
       " 'shootilass': 185024,\n",
       " 'kilohertz': 57989,\n",
       " 'esetonian': 225632,\n",
       " 'pierogis': 123403,\n",
       " 'fugazzi': 163311,\n",
       " 'pertinence': 190930,\n",
       " 'gunbattle': 30342,\n",
       " 'foresaw': 74136,\n",
       " 'syndey': 142576,\n",
       " 'combrink': 245011,\n",
       " 'mahdieh': 222092,\n",
       " 'digawapi': 228196,\n",
       " 'arthropodic': 196391,\n",
       " 'brookes': 149194,\n",
       " 'glycosylated': 255295,\n",
       " 'somozistas': 152121,\n",
       " 'unbridled': 66357,\n",
       " 'manio': 253540,\n",
       " '2723': 199807,\n",
       " 'laghman': 114525,\n",
       " 'eastern': 2189,\n",
       " 'mechanical': 6977,\n",
       " 'landano': 143867,\n",
       " 'naumberg': 109808,\n",
       " 'sawko': 115044,\n",
       " 'guidantus': 261369,\n",
       " 'gilliam': 44327,\n",
       " 'chugging': 9138,\n",
       " 'rezaian': 230415,\n",
       " 'shetawi': 92832,\n",
       " 'nuclear': 1526,\n",
       " 'spokeswman': 114175,\n",
       " 'intolerances': 259787,\n",
       " 'tartter': 193984,\n",
       " 'koumac': 38413,\n",
       " 'gailliard': 123290,\n",
       " 'miekkalina': 95069,\n",
       " 'trammel': 89685,\n",
       " 'plasterboards': 159687,\n",
       " 'b1302': 94412,\n",
       " 'basoco': 172623,\n",
       " 'tows': 70184,\n",
       " 'weve': 599,\n",
       " 'kahlia': 196989,\n",
       " 'tarpaper': 93293,\n",
       " 'steevens': 122097,\n",
       " 'hwc': 200496,\n",
       " 'willpower': 60122,\n",
       " 'forcesand': 250937,\n",
       " 'spewing': 33548,\n",
       " 'righteous': 54389,\n",
       " 'sterba': 237503,\n",
       " 'wewek': 73572,\n",
       " 'kansas': 2293,\n",
       " 'foodbank': 157725,\n",
       " 'barbarians': 29497,\n",
       " 'schmuelling': 13925,\n",
       " 'baileys': 58389,\n",
       " 'reflective': 8071,\n",
       " 'durban': 15381,\n",
       " 'torchlit': 58438,\n",
       " 'incease': 166257,\n",
       " 'pasay': 78890,\n",
       " 'kezsbom': 92026,\n",
       " 'softpedaled': 161271,\n",
       " 'accolades': 53573,\n",
       " 'ultraviolence': 147840,\n",
       " 'clases': 85238,\n",
       " 'selfe': 179267,\n",
       " 'debilitating': 55144,\n",
       " 'boatmen': 101979,\n",
       " 'erxtremely': 144986,\n",
       " 'fortunately': 27192,\n",
       " 'suministro': 215383,\n",
       " 'gombar': 192057,\n",
       " 'abodes': 186514,\n",
       " 'faglier': 93679,\n",
       " 'lod': 77610,\n",
       " 'turkot': 168948,\n",
       " 'woolacott': 236343,\n",
       " 'marshlands': 43153,\n",
       " 'vainglorious': 102885,\n",
       " 'mangent': 103107,\n",
       " 'monohull': 73612,\n",
       " 'whoopee': 37477,\n",
       " 'libration': 135892,\n",
       " 'lemongrass': 193194,\n",
       " 'turkish': 10177,\n",
       " 'infotechs': 57813,\n",
       " 'wilensky': 162316,\n",
       " 'piglets': 31577,\n",
       " 'swierczek': 191549,\n",
       " 'bkosta': 173130,\n",
       " 'mizeners': 256790,\n",
       " 'mcmahons': 75196,\n",
       " 'sunglass': 171715,\n",
       " 'hell': 2828,\n",
       " 'subchaser': 148113,\n",
       " 'sobin': 236109,\n",
       " 'chandlee': 70368,\n",
       " 'shortcomings': 23356,\n",
       " 'gotaris': 118232,\n",
       " '27055': 224680,\n",
       " 'noxell': 118591,\n",
       " 'susbstance': 206486,\n",
       " '8880': 59947,\n",
       " 'anthocyanin': 245018,\n",
       " '-13090': 223790,\n",
       " 'friedland': 45169,\n",
       " 'meggido': 182245,\n",
       " 'kretchmers': 105501,\n",
       " 'recucing': 246483,\n",
       " 'surmounting': 34159,\n",
       " 'tomasyan': 252319,\n",
       " 'frieds': 103773,\n",
       " 'albaby': 243279,\n",
       " 'trumps': 30282,\n",
       " 'irgun': 167635,\n",
       " 'fagiano': 209127,\n",
       " 'cardellas': 109634,\n",
       " 'wadesboro': 190859,\n",
       " 'stymy': 64588,\n",
       " 'luanne': 39651,\n",
       " 'tablitas': 155808,\n",
       " 'winterplace': 47223,\n",
       " 'kbpi': 174084,\n",
       " 'jotta': 238720,\n",
       " 'allusion': 52832,\n",
       " 'stadio': 227518,\n",
       " 'racked': 11854,\n",
       " 'orgega': 123035,\n",
       " 'krasowska': 122674,\n",
       " 'riddet': 123280,\n",
       " 'ponys': 101231,\n",
       " 'croxton': 227964,\n",
       " 'feitler': 79175,\n",
       " 'voltaggio': 208605,\n",
       " 'exterminates': 144269,\n",
       " 'croaking': 79613,\n",
       " 'zagels': 61165,\n",
       " '1662': 117316,\n",
       " 'bernbardino': 251871,\n",
       " 'weissert': 232968,\n",
       " 'polymerase': 103400,\n",
       " 'agriculturals': 224094,\n",
       " 'wouldaying': 231996,\n",
       " 'dome': 24725,\n",
       " 'orbium': 185260,\n",
       " 'maresuke': 122166,\n",
       " 'rumiantsev': 153122,\n",
       " 'makubane': 87968,\n",
       " 'desalvo': 72832,\n",
       " 'tinkling': 93746,\n",
       " 'kilometres': 249327,\n",
       " 'stanclearadaclosindoors': 125981,\n",
       " '-12160': 222964,\n",
       " 'apostel': 157602,\n",
       " 'churched': 40544,\n",
       " 'reaping': 20202,\n",
       " 'olrecycling': 184602,\n",
       " 'alamanac': 184671,\n",
       " 'tadjikistan': 55527,\n",
       " 'blueberries': 43884,\n",
       " 'reginia': 232861,\n",
       " 'rotenburgs': 213854,\n",
       " 'factors': 1813,\n",
       " 'demarche': 86654,\n",
       " 'parliement': 128881,\n",
       " '86495': 229307,\n",
       " 'armstrongs': 66187,\n",
       " 'merchan': 156976,\n",
       " 'mgovernment': 151776,\n",
       " 'maticeks': 214071,\n",
       " 'containerization': 116327,\n",
       " 'shiawassee': 225544,\n",
       " 'corgard': 172620,\n",
       " 'ozcan': 241610,\n",
       " 'kihss': 219066,\n",
       " 'invisibility': 84558,\n",
       " 'seeminginly': 161370,\n",
       " 'reiterating': 22591,\n",
       " 'pompon': 103621,\n",
       " 'kenichiro': 141392,\n",
       " 'quiguyan': 196624,\n",
       " 'hefty': 18258,\n",
       " 'prty': 36285,\n",
       " 'indefatigable': 81479,\n",
       " 'handmaids': 85219,\n",
       " 'sengram': 264248,\n",
       " 'tudsbury': 258710,\n",
       " 'clams': 40628,\n",
       " 'goeb': 236815,\n",
       " 'incarnation': 37422,\n",
       " 'leivas': 243256,\n",
       " 'colkins': 54834,\n",
       " 'upsets': 32240,\n",
       " 'galella': 214296,\n",
       " 'gumedes': 129319,\n",
       " 'bhantura': 223835,\n",
       " 'teabags': 129997,\n",
       " 'turnabout': 47467,\n",
       " 'lunchrooms': 72414,\n",
       " 'beamishs': 138888,\n",
       " 'plaths': 104833,\n",
       " 'turbulance': 142990,\n",
       " 'kitajimas': 252418,\n",
       " 'zamzam': 98344,\n",
       " 'sobhaninia': 84447,\n",
       " 'kihei': 195880,\n",
       " 'chimura': 149511,\n",
       " 'lalumandier': 266697,\n",
       " 'taa': 56649,\n",
       " 'swopes': 265018,\n",
       " 'syrupy': 101957,\n",
       " 'promininet': 61574,\n",
       " 'impaired': 4077,\n",
       " 'bernardines': 105459,\n",
       " 'motorcycles': 21706,\n",
       " 'duncombe': 85246,\n",
       " 'safely': 2303,\n",
       " 'pingi': 250147,\n",
       " 'easy': 5443,\n",
       " 'unisex': 17341,\n",
       " 'wilkey': 28711,\n",
       " 'vaniwaarden': 208388,\n",
       " 'asef': 167094,\n",
       " 'encloses': 124203,\n",
       " 'bendano': 114481,\n",
       " 'abetters': 185059,\n",
       " 'reflexes': 25399,\n",
       " 'tevlin': 219476,\n",
       " 'muskokee': 252536,\n",
       " 'megapubs': 145219,\n",
       " 'taubeneck': 137880,\n",
       " 'boomingest': 103111,\n",
       " 'koniuchowsky': 157589,\n",
       " 'naugahyde': 77624,\n",
       " 'oldsmobiles': 37867,\n",
       " 'chungbu': 110776,\n",
       " 'strenghth': 82466,\n",
       " 'coppells': 194522,\n",
       " 'funafuti': 17430,\n",
       " 'intra': 14348,\n",
       " 'fornicating': 142073,\n",
       " 'courageousness': 35789,\n",
       " 'mano': 136791,\n",
       " 'alignment': 30657,\n",
       " 'limping': 23535,\n",
       " 'playability': 178946,\n",
       " 'pyramiding': 129651,\n",
       " 'kisan': 124655,\n",
       " 'krupa': 161785,\n",
       " '352': 23832,\n",
       " 'nel': 58056,\n",
       " 'sirhan': 19661,\n",
       " 'scioto': 52663,\n",
       " '2675': 115668,\n",
       " 'cliosphic': 182964,\n",
       " 'gimar': 141059,\n",
       " 'gourde': 93048,\n",
       " 'elks': 20218,\n",
       " 'kuankachorn': 93502,\n",
       " 'overexposure': 61513,\n",
       " 'cicek': 221918,\n",
       " 'bentsur': 153568,\n",
       " 'nootka': 189861,\n",
       " 'bandells': 208266,\n",
       " 'honoaria': 181807,\n",
       " 'pactel': 127707,\n",
       " 'afafat': 180573,\n",
       " 'kahlil': 96485,\n",
       " 'tremble': 37133,\n",
       " 'kountche': 100647,\n",
       " 'schroeder': 12162,\n",
       " 'unsteadiness': 166369,\n",
       " 'englanders': 44632,\n",
       " 'argueta': 120089,\n",
       " 'gladder': 108763,\n",
       " 'felmy': 106945,\n",
       " 'akthar': 227989,\n",
       " 'upius': 257729,\n",
       " 'wvca': 254489,\n",
       " 'portrayed': 5115,\n",
       " '558': 20809,\n",
       " 'stephano': 250216,\n",
       " 'werd': 206030,\n",
       " 'mashkan': 148558,\n",
       " 'riesgos': 215359,\n",
       " 'slyke': 183610,\n",
       " 'streety': 213440,\n",
       " 'nabers': 50629,\n",
       " 'ublick': 262802,\n",
       " 'fresfilm': 260460,\n",
       " 'quaylequalified': 264426,\n",
       " 'rankins': 249567,\n",
       " 'kancamagus': 198787,\n",
       " 'olefine': 233799,\n",
       " 'kochetov': 158137,\n",
       " 'lateran': 19706,\n",
       " 'zuno': 22030,\n",
       " 'imperatriz': 20433,\n",
       " 'mucilage': 166453,\n",
       " 'ebbitt': 121388,\n",
       " 'legible': 62671,\n",
       " 'arbiso': 139192,\n",
       " 'reenacted': 97594,\n",
       " 'forgotton': 40651,\n",
       " 'mountaintops': 65387,\n",
       " 'makaha': 172270,\n",
       " 'examinees': 249379,\n",
       " 'afflicts': 42069,\n",
       " 'modulates': 69086,\n",
       " 'torpey': 132517,\n",
       " 'neodys': 142100,\n",
       " 'hydraulics': 26189,\n",
       " 'jarvey': 149913,\n",
       " 'cavernosal': 262810,\n",
       " 'prezelski': 120487,\n",
       " 'belardo': 87364,\n",
       " 'unimaginably': 117755,\n",
       " 'ineligble': 131401,\n",
       " 'mugla': 200555,\n",
       " 'hypocricy': 257810,\n",
       " 'jersy': 35819,\n",
       " 'tomiji': 247180,\n",
       " 'brashest': 240700,\n",
       " 'bukowskis': 96972,\n",
       " 'repairable': 123240,\n",
       " 'annunzio': 6993,\n",
       " 'muhjahedeen': 223412,\n",
       " 'registan': 121478,\n",
       " 'hepatpresses': 242088,\n",
       " 'issions': 205667,\n",
       " 'kerkin': 149939,\n",
       " 'homeporting': 243890,\n",
       " 'appearently': 122560,\n",
       " 'cholera': 22078,\n",
       " 'azoff': 23975,\n",
       " 'mouwasat': 255751,\n",
       " 'tetsuyoshi': 85996,\n",
       " '2104': 259227,\n",
       " 'harran': 110678,\n",
       " 'prussic': 92079,\n",
       " 'poxy': 260920,\n",
       " 'fallklands': 169155,\n",
       " 'fintech': 77683,\n",
       " 'mauney': 112889,\n",
       " 'confired': 117961,\n",
       " 'briefly': 5572,\n",
       " 'takutsaka': 115505,\n",
       " 'jorma': 87905,\n",
       " 'colliding': 3101,\n",
       " 'teborek': 226800,\n",
       " 'exprimental': 202419,\n",
       " 'bureucratic': 134934,\n",
       " 'bottaro': 244361,\n",
       " 'volum': 192594,\n",
       " 'apologist': 74642,\n",
       " 'pockington': 226551,\n",
       " 'abergavenny': 32042,\n",
       " 'freeze': 3558,\n",
       " 'troubadours': 122358,\n",
       " 'foisted': 83188,\n",
       " 'kohlmeyer': 200310,\n",
       " 'hiestand': 64412,\n",
       " 'castellini': 25374,\n",
       " '-13327': 212595,\n",
       " 'reguees': 90376,\n",
       " '0560': 48291,\n",
       " 'rhions': 219605,\n",
       " 'coauthored': 128790,\n",
       " 'inconsequential': 83989,\n",
       " 'shabtai': 71319,\n",
       " 'pronounced': 2869,\n",
       " '2807': 252834,\n",
       " 'naif': 56930,\n",
       " 'pulaksi': 230525,\n",
       " 'indentured': 66311,\n",
       " 'disbelievers': 148355,\n",
       " 'thamert': 112076,\n",
       " 'janneys': 258606,\n",
       " 'tyanna': 234137,\n",
       " 'latecomers': 32277,\n",
       " 'mnctive': 161391,\n",
       " 'deteriorate': 20450,\n",
       " 'reltok': 227615,\n",
       " 'kears': 105013,\n",
       " 'channey': 218681,\n",
       " '-1939': 84076,\n",
       " 'poults': 224720,\n",
       " 'hayesod': 195298,\n",
       " 'subdistricts': 39126,\n",
       " 'leanness': 127065,\n",
       " 'bongos': 166626,\n",
       " 'snoy': 106181,\n",
       " 'newarks': 83560,\n",
       " 'criminalists': 98798,\n",
       " 'sketchily': 222719,\n",
       " '1125a': 178977,\n",
       " 'atocha': 77760,\n",
       " 'kyls': 208000,\n",
       " 'surmise': 34078,\n",
       " 'mokotos': 52854,\n",
       " 'mismangement': 78901,\n",
       " 'tok8': 132262,\n",
       " 'hedley': 40105,\n",
       " 'tanning': 42367,\n",
       " 'festooning': 188746,\n",
       " 'baketball': 83778,\n",
       " 'containes': 190629,\n",
       " 'dyal': 195504,\n",
       " 'jagy': 128005,\n",
       " 'pipher': 256063,\n",
       " 'liftoff': 10055,\n",
       " 'touati': 201747,\n",
       " 'rosewoods': 178947,\n",
       " 'emmies': 170189,\n",
       " 'cavities': 47259,\n",
       " 'markovics': 43219,\n",
       " 'tereshkova': 80982,\n",
       " 'zeimetz': 241192,\n",
       " 'beighle': 19182,\n",
       " 'skurnik': 180002,\n",
       " 'detach': 53320,\n",
       " 'stollman': 156623,\n",
       " '88175': 154845,\n",
       " 'committeesconcludes': 158946,\n",
       " 'byalek': 263698,\n",
       " 'extremists': 10192,\n",
       " '3652': 208451,\n",
       " 'linder': 101803,\n",
       " 'dynamite': 26830,\n",
       " 'prematurely': 16277,\n",
       " 'thiep': 167463,\n",
       " 'hyrum': 149814,\n",
       " 'harith': 63672,\n",
       " 'spotswood': 204309,\n",
       " 'reinsurance': 82504,\n",
       " 'monring': 168576,\n",
       " 'malinsky': 206061,\n",
       " 'bellinhausen': 76169,\n",
       " 'veeco': 195485,\n",
       " 'derail': 30140,\n",
       " 'airpockets': 110471,\n",
       " 'salvadoe': 132361,\n",
       " 'collyer': 230212,\n",
       " 'hida': 132291,\n",
       " 'sarron': 160633,\n",
       " 'reassignments': 28956,\n",
       " 'paveletsky': 78032,\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_dict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing corpus\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4a174f7c8cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_token_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgensim_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-4a174f7c8cfc>\u001b[0m in \u001b[0;36mget_corpus\u001b[0;34m(gensim_dict, corpus_file)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading existing corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating corpus, and saving....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_token_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyir/lib/python3.5/site-packages/gensim-3.2.0-py3.5-linux-x86_64.egg/gensim/corpora/mmcorpus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# avoid calling super(), too confusing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mIndexedCorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyir/lib/python3.5/site-packages/gensim-3.2.0-py3.5-linux-x86_64.egg/gensim/matutils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input, transposed)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransposed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"
     ]
    }
   ],
   "source": [
    "def get_corpus(gensim_dict, corpus_file=\"ir1.mm\"):\n",
    "    if os.path.exists(corpus_file):\n",
    "        print(\"Loading existing corpus\")\n",
    "        return gensim.corpora.MmCorpus(corpus_file)\n",
    "    print(\"Creating corpus, and saving....\")\n",
    "    gensim.corpora.MmCorpus.serialize(corpus_file, (gensim_dict.doc2bow(doc) for doc in document_token_iterator()))\n",
    "    return gensim.corpora.MmCorpus(corpus_file)\n",
    "gensim_corpus = get_corpus(gensim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4a1afb05b29c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlsi_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlsimodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgensim_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgensim_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "num_topics = 256\n",
    "lsi_model = gensim.models.lsimodel.LsiModel(corpus=gensim_corpus, id2word=gensim_dict, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_model = gensim.models.lsimodel.LsiModel.load(\"lsi.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_model.save(\"lsi.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsi_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_model[dictionary.doc2bow(['washington'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "cosine_distance_vectorized = np.vectorize(cosine_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSMScorer:\n",
    "    def __init__(self, corpus, dictionary, model):\n",
    "        self.model = model\n",
    "        self.corpus = corpus\n",
    "        self.dict = dictionary\n",
    "        \n",
    "        self.document_vectors = np.zeros((num_documents, self.model.num_topics))\n",
    "        self.document_index = []\n",
    "        for idx, int_doc_id in enumerate(range(index.document_base(), index.maximum_document())):\n",
    "            ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "            document_tokens = [id2token[token_id].lower() for token_id in doc_token_ids if token_id > 0]\n",
    "            self.document_index.append(ext_doc_id)\n",
    "            self.document_vectors[idx] = self._get_vector(document_tokens)    \n",
    "    \n",
    "    def _get_vector(self, tokens):\n",
    "        vector = np.zeros(self.model.num_topics)\n",
    "        # TODO why isn't the size always = num_topics? It's super strange! \n",
    "        # TODO check back why this is happening. it's a serious problem\n",
    "        for topic, value in self.model[self.dict.doc2bow(tokens)]:\n",
    "            vector[topic] = value\n",
    "        return vector\n",
    "    \n",
    "    def score(self, query_tokens):\n",
    "        # TODO order = 1 same, -1 not same\n",
    "        # TODO cehck others as well\n",
    "        # TODO some values are \n",
    "        # TODO have to use JS divergence\n",
    "        query_vector = self._get_vector(query_tokens)\n",
    "        distances = np.apply_along_axis(lambda _: cosine_distance(query_vector, _), 1, self.document_vectors)\n",
    "        #for ext_doc_id, document_vector in self.document_vectors.items():\n",
    "        #    results.append((cosine_distance(document_vector, query_vector), ext_doc_id))\n",
    "        results = list(zip(distances, self.document_index))\n",
    "        results.sort(key=lambda _: -_[0])\n",
    "        return results\n",
    "\n",
    "lsi_scorer = LSMScorer(gensim_corpus, gensim_dict, lsi_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval_lsm(model_name, scorer):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param scorer: the LSMScorer \n",
    "    \"\"\"\n",
    "    \n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    #if os.path.exists(run_out_path):\n",
    "    #    return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    \n",
    "    data = {}\n",
    "    start_time = time.time()\n",
    "    for index, (query_id, query_tokens) in enumerate(tokenized_queries.items()):\n",
    "        query_tokens = [id2token[token_id].lower() for token_id in query_tokens if token_id > 0]\n",
    "        data[query_id] = lsi_scorer.score(query_tokens)\n",
    "        if (index + 1) % 10 == 0:\n",
    "            print(\"\\t\", index + 1 , \"done.\", time.time() - start_time, \"elapsed\" )\n",
    "\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_retrieval_lsm(\"lsi\", lsi_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trec.evaluate(\"./ap_88_89/qrel_validation\", \"lsi.run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyndri.Index of 164597 documents>\n",
      "Loading vocabulary.\n",
      "Initializing word2vec.\n",
      "Constructing word2vec vocabulary.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-5ccf238c643c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Constructing word2vec vocabulary.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mword2vec_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2vec_init\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyir/lib/python3.5/site-packages/gensim-3.2.0-py3.5-linux-x86_64.egg/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0municode\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyir/lib/python3.5/site-packages/gensim-3.2.0-py3.5-linux-x86_64.egg/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyir/lib/python3.5/site-packages/pyndri/compat.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m             yield tuple(\n\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 if token_id > 0 and token_id in self.dictionary)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyir/lib/python3.5/site-packages/pyndri/compat.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 if token_id > 0 and token_id in self.dictionary)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyir/lib/python3.5/site-packages/pyndri/dictionary.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, token_id)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "print(index)\n",
    "\n",
    "print('Loading vocabulary.')\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "print('Initializing word2vec.')\n",
    "word2vec_init = gensim.models.Word2Vec(\n",
    "    size=300,  # Embedding size\n",
    "    window=5,  # One-sided window size\n",
    "    sg=True,  # Skip-gram.\n",
    "    min_count=5,  # Minimum word frequency.\n",
    "    sample=1e-3,  # Sub-sample threshold.\n",
    "    hs=False,  # Hierarchical softmax.\n",
    "    negative=10,  # Number of negative examples.\n",
    "    iter=1,  # Number of iterations.\n",
    "    workers=8,  # Number of workers.\n",
    ")\n",
    "\n",
    "print('Constructing word2vec vocabulary.')\n",
    "word2vec_init.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "models = [word2vec_init]\n",
    "\n",
    "for epoch in range(1, 5 + 1):\n",
    "\n",
    "    print('Epoch %d', epoch)\n",
    "    model = copy.deepcopy(models[-1])\n",
    "    \n",
    "    '''\n",
    "    Any direct calls to method train() of Word2Vec/Doc2Vec now require an explicit epochs parameter and explicit estimate of corpus size. \n",
    "    The most usual way to call train is vec_model.train(sentences, total_examples=self.corpus_count, epochs=self.iter)\n",
    "    '''\n",
    "    #model.train(sentences)\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    models.append(model)\n",
    "\n",
    "print('Trained models: %s', models)\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "# the total number of documents\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "# tokenize the queries\n",
    "# query_id --> [token_id1, token_id2, ...] \n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "# gather all of the query token_ids into a set TODO why? \n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
