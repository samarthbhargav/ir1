{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import enum\n",
    "import itertools\n",
    "import operator\n",
    "import collections\n",
    "import math\n",
    "import codecs\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain why we chose HR=2, R=1, N=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@enum.unique\n",
    "class Grade(enum.IntEnum):\n",
    "    \"\"\"\n",
    "    Represents relevance on a graded scale. \n",
    "    Any positive number represents relevance, with higher numbers representing higher relevance (total ordering)\n",
    "    Zero represents irrelevant\n",
    "    \"\"\"\n",
    "    N = (0)   # Not relevant\n",
    "    R = (1)   # Relevant\n",
    "    HR = (2)  # Highly relevant\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def is_relevant(self):\n",
    "        \"\"\"\n",
    "        'Binarizes' the relevance. Useful for computing binary metrics like precision / recall\n",
    "        \"\"\"\n",
    "        return False if self.value == 0 else True\n",
    "    \n",
    "    @classmethod\n",
    "    def from_int_list(cls, int_list):\n",
    "        \"\"\"\n",
    "        Converts a list of integers to a list of Grade\n",
    "        \"\"\"\n",
    "        members = dict((member.value, member) for member in cls.__members__.values())\n",
    "        return [members[value] for value in int_list]\n",
    "\n",
    "    @classmethod\n",
    "    def max_grade(cls):\n",
    "        return max(member.value for member in cls.__members__.values())\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "\n",
    "P: {N N N N N}\n",
    "E: {N N N N R}\n",
    "\n",
    "â€¦\n",
    "P: {HR HR HR HR R}\n",
    "E: {HR HR HR HR HR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_permutations(grade, sequence_length):\n",
    "    \"\"\"\n",
    "    For a given grade and a sequence length n, returns all possible permutations of length n that can be formed. \n",
    "    Note that a generator is returned and not a list\n",
    "    \"\"\"\n",
    "    for i, perm in enumerate(itertools.product(range(len(grade.__members__)), repeat=sequence_length)):\n",
    "        yield grade.from_int_list(perm)\n",
    "\n",
    "def get_all_permutation_pairs(grade, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates all possible pairs of permutations given a grade and a sequence length.\n",
    "    Note that a generator is returned and not a list\n",
    "    \"\"\"\n",
    "    sequence_1 = get_all_permutations(grade, sequence_length)\n",
    "    sequence_2 = get_all_permutations(grade, sequence_length)    \n",
    "    for pair in itertools.product(sequence_1, sequence_2):\n",
    "        yield pair\n",
    "        \n",
    "def count_sequence(seq):\n",
    "    \"\"\"\n",
    "    `len` for generators\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for _ in seq:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# A sanity check to ensure the correct number of sequences are being generated\n",
    "# the `-(3**5)` removes exact duplicates\n",
    "assert count_sequence(get_all_permutation_pairs(Grade, 5)) == (3**5) * (3**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO add assumption that we weed out exactly similar documents (or do WE?) dun dun dun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutations = np.array(list(get_all_permutation_pairs(Grade, 5)), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[N, N, N, N, N],\n",
       "        [N, N, N, N, N]],\n",
       "\n",
       "       [[N, N, N, N, N],\n",
       "        [N, N, N, N, R]],\n",
       "\n",
       "       [[N, N, N, N, N],\n",
       "        [N, N, N, N, HR]],\n",
       "\n",
       "       ..., \n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, N]],\n",
       "\n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, R]],\n",
       "\n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, HR]]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[N, N, N, N, N],\n",
       "       [N, N, N, N, N]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[HR, HR, HR, HR, HR],\n",
       "       [HR, HR, HR, HR, HR]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "production = np.array(list(map(operator.itemgetter(0), permutations)))\n",
    "experiment = np.array(list(map(operator.itemgetter(1), permutations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain this in detail. It's one of the central assumptions we make. \n",
    "Single Query Assumption.\n",
    "We can assume that the entire experiment is for a single query, so there are only 10 R + 10 HR documents i.e a total of 20 relevant documents.\n",
    "\n",
    "Comment by Ece: Isn't this already given in the description along with the idea that the returned documents will be unique? Our assumption was more in the line of supposing that there are 10 irrelevant documents in the collection, given query q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO double check this\n",
    "relevant_count_production = 20\n",
    "relevant_count_experiment = 20\n",
    "(relevant_count_experiment, relevant_count_production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Evaluation Measures (10 points)\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    0.    0.   ...,  0.25  0.25  0.25]\n"
     ]
    }
   ],
   "source": [
    "class Metric:\n",
    "    \n",
    "    \"\"\"\n",
    "    Common parent for all metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def name(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        \"\"\" TODO \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Precision(Metric):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"precision\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(results))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        # binarize, compute how many are relevant and divide by k\n",
    "        return sum(r.is_relevant for r in results) / k\n",
    "    \n",
    "class Recall(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"recall\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"relevant_count\" in kwargs\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(production))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        # binarize and count, divide by number of relevant documents\n",
    "        # the number of relevant documents is a function of the total number of relevant documents\n",
    "        # and how many *can* be present in a list of length k\n",
    "        # if there are only k = 5 and total = 20, recall can never be 1 even when the all documents are relevant\n",
    "        # therefore, the denom should be min(k, total)\n",
    "        # TODO verify\n",
    "        return sum(r.is_relevant for r in results) / kwargs[\"relevant_count\"] \n",
    "        \n",
    "    \n",
    "class AveragePrecision(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"relevant_count\" in kwargs\n",
    "        P = Precision()\n",
    "        precisions = np.zeros(len(results))\n",
    "        for k in range(1, len(results) + 1):\n",
    "            if(str(results[k-1]) != 'N'):\n",
    "                precisions[k - 1] = P.compute(results, k = k)\n",
    "        # TODO verify\n",
    "        return precisions.sum() / kwargs[\"relevant_count\"] \n",
    "\n",
    "AP = AveragePrecision()\n",
    "ap = np.array([AP.compute(r, relevant_count = relevant_count_production) for r in production])\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13648148148148148"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Explain why the max in our case can only be 25, so this shit is right!\n",
    "\n",
    "Comment by Ece: I think the calculation of average precision only considers precision values at ranks where relevant documents are returned, does the method above take it into consideration? I might be missing something, but it seems to me that it adds up all the precision values. JUST ADDED THE IF STATEMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1701.,  4374.,  6318.,  6804.,  6804.,  9720.,  3888.,  7776.,\n",
       "         3888.,  7776.]),\n",
       " array([ 0.   ,  0.025,  0.05 ,  0.075,  0.1  ,  0.125,  0.15 ,  0.175,\n",
       "         0.2  ,  0.225,  0.25 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff8e09645c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HR, HR, HR, HR, HR, HR, HR, HR, HR, HR, R, R, R, R, R, R, R, R, R, R, N, N, N, N, N, N, N, N, N, N]\n"
     ]
    }
   ],
   "source": [
    "class DCG(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        \n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(production))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        \n",
    "        dcg = 0.0 \n",
    "        for rank, result in enumerate(results):\n",
    "            rank += 1 # TODO verify\n",
    "            gain = 2**result.value - 1\n",
    "            discount = math.log2(1 + rank)\n",
    "            dcg += gain / discount\n",
    "        return dcg\n",
    "\n",
    "\n",
    "class NDCG(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "\n",
    "    # NDCG: We considered two possibilities. TODO explain it\n",
    "    # Normalization is done according to the slides mentioned here \n",
    "    # https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"ideal_ranking\" in kwargs\n",
    "        \n",
    "        dcg = DCG()\n",
    "        \n",
    "        ideal_ranking = kwargs[\"ideal_ranking\"]\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(results))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        ideal_ranking = ideal_ranking[:k]\n",
    "        \n",
    "        max_dcg = dcg.compute(ideal_ranking)\n",
    "        \n",
    "        return NDCG.dcg.compute(results) / max_dcg\n",
    "\n",
    "ideal_ranking = [Grade.HR] * 10 + [Grade.R] * 10 + [Grade.N] * 10\n",
    "print(ideal_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.84375\n",
      "0.859375\n",
      "0.8623046875\n",
      "0.8623046875\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.1875\n",
      "0.1875\n"
     ]
    }
   ],
   "source": [
    "class ERR(Metric):\n",
    "    # TODO cite this http://delivery.acm.org/10.1145/1650000/1646033/p621-chapelle.pdf?ip=145.109.35.88&id=1646033&acc=ACTIVE%20SERVICE&key=0C390721DC3021FF%2E86041C471C98F6DA%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=1026312371&CFTOKEN=18042814&__acm__=1515748872_543fbe0fe6a5c9d1fadb8f8341c9da3c\n",
    "    # TODO and explain how you converted to code\n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"grade\" in kwargs\n",
    "        grade = kwargs[\"grade\"]\n",
    "        \n",
    "        def r_func(result):\n",
    "            return ((2 ** result.value) - 1) / (2 ** grade.max_grade())  \n",
    "        \n",
    "        err = 0.0\n",
    "        for rank, result in enumerate(results):\n",
    "            previous_results = results[:rank]\n",
    "            satisfaction = 1\n",
    "            for previous_result in previous_results:\n",
    "                satisfaction = satisfaction * ((1 - r_func(previous_result)))\n",
    "            # rank is 0-indexed here, so add 1\n",
    "            # I am not sure, but I think r_func(result) should be here. Please check TODO\n",
    "            err += (1/(rank + 1)) * satisfaction * r_func(result)\n",
    "            print(err)\n",
    "        return err\n",
    "# TODO this is incorrect. I couldn't figure out what is going wrong\n",
    "\n",
    "# Comment by Ece: The part where you multiply with r_func(result) seems correct, when compared to slides by Ilya.\n",
    "# I failed to see how this is incorrect, could you explain why?\n",
    "# https://lingpipe-blog.com/2010/03/09/chapelle-metzler-zhang-grinspan-2009-expected-reciprocal-rank-for-graded-relevance/\n",
    "# http://don-metzler.net/presentations/err_cikm09.pdf\n",
    "\n",
    "err = ERR()\n",
    "print(err.compute([Grade.HR, Grade.HR, Grade.HR, Grade.HR], grade=Grade))\n",
    "print(err.compute([Grade.N, Grade.N, Grade.N, Grade.HR], grade=Grade))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the ð›¥measure (0 points)\n",
    "\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ð›¥measure = measureE-measureP. Consider only those pairs for which E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27954"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision = AveragePrecision()\n",
    "ndcg = NDCG() # TODO or pick ERR?\n",
    "\n",
    "# TODO which measure do we use to subset? All 3 or just one? right now I\"m doing only one\n",
    "\n",
    "average_precision_P = np.array([average_precision.compute(results, relevant_count = relevant_count_production) \\\n",
    "                                for results in production])\n",
    "average_precision_E = np.array([average_precision.compute(results, relevant_count = relevant_count_experiment) \\\n",
    "                                for results in experiment])\n",
    "delta_measure = average_precision_E - average_precision_P\n",
    "len(permutations[delta_measure > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutations = permutations[delta_measure > 0]\n",
    "production = production[delta_measure > 0]\n",
    "experiment = experiment[delta_measure > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Interleaving (15 points)\n",
    "Implement 2 interleaving algorithms: \n",
    "(1) Team-Draft Interleaving OR Balanced Interleaving, ~~AND (2), Probabilistic Interleaving.~~ \n",
    "\n",
    "The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toss_coin(heads = 0.5):\n",
    "    # head = True, Tails is False\n",
    "    return True if random.random() < heads else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain\n",
    "\n",
    "Citations for both algorithms: \n",
    "\n",
    "https://www.cs.cornell.edu/people/tj/publications/radlinski_etal_08b.pdf\n",
    "\n",
    "https://www.cs.cornell.edu/people/tj/publications/chapelle_etal_12a.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interleaved(source='A', source_index=0, result=R),\n",
       " Interleaved(source='B', source_index=0, result=R),\n",
       " Interleaved(source='A', source_index=1, result=HR),\n",
       " Interleaved(source='B', source_index=1, result=N),\n",
       " Interleaved(source='A', source_index=2, result=N),\n",
       " Interleaved(source='B', source_index=2, result=HR)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Interleaved = collections.namedtuple(\"Interleaved\", [\"source\", \"source_index\", \"result\"])\n",
    "        \n",
    "class BalancedInterleaving:\n",
    "    \n",
    "    @staticmethod\n",
    "    def interleave(A, B, A_name, B_name):\n",
    "\n",
    "        selected = toss_coin()\n",
    "        interleaved = []\n",
    "        \n",
    "        ka = 0\n",
    "        kb = 0\n",
    "        \n",
    "        while(ka <= len(A) and kb <= len(B) and (ka < len(A) or kb < len(B))):\n",
    "\n",
    "            if (ka < kb) or (ka==kb and selected):\n",
    "                    tup = Interleaved(A_name, ka, A[ka])\n",
    "                    # assumption: all docs are unique, most recently picked document is not in the interleaved list\n",
    "                    interleaved.append(tup)\n",
    "                    ka += 1\n",
    "\n",
    "            else:  \n",
    "                    tup = Interleaved(B_name, kb, B[kb])\n",
    "                    # assumption: all docs are unique, most recently picked document is not in the interleaved list\n",
    "                    interleaved.append(tup)\n",
    "                    kb += 1\n",
    "        \n",
    "        return interleaved\n",
    "    \n",
    "    @staticmethod\n",
    "    def score_clicks(interleaved, clicks):\n",
    "        scores = collections.defaultdict(int)\n",
    "        \n",
    "        #assumption: all docs are unique, just count the clicks belonging to each algorithm\n",
    "\n",
    "        for c in clicks:\n",
    "            il = interleaved[c]\n",
    "            scores[il.source] += 1\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "A = [Grade.R, Grade.HR, Grade.N]\n",
    "B = [Grade.R, Grade.N, Grade.HR]\n",
    "\n",
    "interleaved = BalancedInterleaving.interleave(A, B, \"A\", \"B\")\n",
    "interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'A': 3})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks = [0, 2, 4]\n",
    "BalancedInterleaving.score_clicks(interleaved, clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interleaved(source='A', source_index=0, result=R),\n",
       " Interleaved(source='B', source_index=0, result=R),\n",
       " Interleaved(source='B', source_index=1, result=N),\n",
       " Interleaved(source='A', source_index=1, result=HR),\n",
       " Interleaved(source='B', source_index=2, result=HR),\n",
       " Interleaved(source='A', source_index=2, result=N)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TeamDraftInterleaving:\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def interleave(A, B, A_name, B_name):\n",
    "\n",
    "        interleaved = []\n",
    "        TeamA = set()\n",
    "        TeamB = set()\n",
    "\n",
    "        ka = 0\n",
    "        kb = 0\n",
    "\n",
    "        while(ka <= len(A) and kb <= len(B)):\n",
    "            selected = toss_coin()\n",
    "            if ka == len(A) and kb == len(B):\n",
    "                break\n",
    "            elif(len(TeamA) < len(TeamB)) or (len(TeamA) == len(TeamB) and selected == 0):\n",
    "                tup = Interleaved(A_name, ka, A[ka])\n",
    "                interleaved.append(tup)\n",
    "                TeamA.add(tup)\n",
    "                ka += 1\n",
    "            else:\n",
    "                tup = Interleaved(B_name, kb, B[kb])\n",
    "                interleaved.append(tup)\n",
    "                TeamB.add(tup)\n",
    "                kb +=1\n",
    "               \n",
    "        return interleaved\n",
    "\n",
    "    @staticmethod\n",
    "    def score_clicks(interleaved, clicks):\n",
    "        scores = collections.defaultdict(int)\n",
    "        \n",
    "        for c in clicks:\n",
    "            il = interleaved[c]\n",
    "            scores[il.source] += 1\n",
    "            \n",
    "        return scores\n",
    "                \n",
    "A = [Grade.R, Grade.HR, Grade.N]\n",
    "B = [Grade.R, Grade.N, Grade.HR]\n",
    "\n",
    "interleaved = TeamDraftInterleaving.interleave(A, B, \"A\", \"B\")\n",
    "interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sessions:  11717\n",
      "Total number of interactions:  100000\n"
     ]
    }
   ],
   "source": [
    "ClickAction = collections.namedtuple(\"ClickAction\", [\"clicked_id\"])\n",
    "QueryAction = collections.namedtuple(\"QueryAction\", [\"query_id\", \"results\"])\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, session_id):\n",
    "        self.session_id = session_id\n",
    "        self.data = []\n",
    "    \n",
    "    @property\n",
    "    def clicks(self):\n",
    "        return list(filter(lambda _: isinstance(_, ClickAction), self.data))\n",
    "    \n",
    "    @property\n",
    "    def queries(self):\n",
    "        return list(filter(lambda _: isinstance(_, QueryAction), self.data))\n",
    "    \n",
    "    def get_complete_interactions(self):\n",
    "        # return complete interactions\n",
    "        last_query = 0\n",
    "        for index, action in enumerate(self.data):\n",
    "            if index != 0 and isinstance(action, QueryAction):\n",
    "                yield self.data[last_query: index]\n",
    "                last_query = index\n",
    "        if isinstance(self.data[-1], ClickAction):\n",
    "            yield self.data[last_query:]\n",
    "        \n",
    "class YandexData:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.sessions = []\n",
    "        self.u = 0\n",
    "        self.q = 0\n",
    "        self._read()\n",
    "        \n",
    "    def _read(self):\n",
    "        with codecs.open(self.file_path, \"r\", \"utf-8\") as reader:\n",
    "            current_session = Session(0)\n",
    "            for line in reader:\n",
    "                line = line.split(\"\\t\")\n",
    "                session_id = int(line[0])\n",
    "                interaction_type = line[2]\n",
    "                if current_session.session_id != session_id:\n",
    "                    self.sessions.append(current_session)\n",
    "                    current_session = Session(session_id)\n",
    "                \n",
    "                if interaction_type == \"C\":\n",
    "                    current_session.data.append(ClickAction(int(line[3])))\n",
    "                else:\n",
    "                    results = [int(q_id) for q_id in line[5:]]\n",
    "                    self.u = max(self.u, max(results))\n",
    "                    query_id = int(line[3])\n",
    "                    self.q = max(self.q, query_id)\n",
    "                    current_session.data.append(QueryAction(query_id, results))\n",
    "        \n",
    "        self.sessions.append(current_session)\n",
    "\n",
    "\n",
    "yandex_data = YandexData(\"./YandexRelPredChallenge.txt\")\n",
    "print(\"Total number of sessions: \", len(yandex_data.sessions))\n",
    "print(\"Total number of interactions: \", sum(len(session.data) for session in yandex_data.sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13445559411047547"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomClickModel:\n",
    "    def __init__(self):\n",
    "        self.rho = 0\n",
    "    \n",
    "    def estimate(self, yandex_data):\n",
    "        clicks = 0\n",
    "        results = 0\n",
    "        for session in yandex_data.sessions:\n",
    "            clicks += len(session.clicks)\n",
    "            results += sum(len(query.results) for query in session.queries)\n",
    "        self.rho = clicks / results\n",
    "    \n",
    "    def predict(self, clicks):\n",
    "        #(b) there is a method that predicts the click probability given a ranked list of relevance labels,\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def simulate(results, grade_to_probability, params):\n",
    "        rho = params.get(\"rho\")\n",
    "        assert 0 <= rho <= 1.0\n",
    "        \n",
    "        # TODO something is wrong with this and I can't figure it out. pls fix \n",
    "        clicks = []\n",
    "        for rank, result in enumerate(results):\n",
    "            click = toss_coin(rho)\n",
    "            if click:\n",
    "                clicks.append(rank)\n",
    "        return clicks\n",
    "\n",
    "rcm = RandomClickModel()\n",
    "rcm.estimate(yandex_data)\n",
    "rcm.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527494, 66579)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yandex_data.u, yandex_data.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDynamicBayesianNetwork:\n",
    "        \n",
    "    def _estimate_sigma(self, yandex_data):\n",
    "        satisfied_clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        total_clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        \n",
    "        for session in yandex_data.sessions:\n",
    "            for interaction in session.get_complete_interactions():\n",
    "                #print(interaction) TODO explain why we continue\n",
    "                if not isinstance(interaction[-1], ClickAction):\n",
    "                    continue\n",
    "                query_action = interaction[0]\n",
    "                # sanity check TODO remove\n",
    "                assert isinstance(query_action, QueryAction)\n",
    "                for click in interaction[1:]:\n",
    "                    # sanity check TODO remove\n",
    "                    assert isinstance(click, ClickAction)\n",
    "                    total_clicks[click.clicked_id][query_action.query_id] += 1\n",
    "                \n",
    "                last_click = interaction[-1]\n",
    "                #print(last_click)\n",
    "                satisfied_clicks[last_click.clicked_id][query_action.query_id] += 1\n",
    "        \n",
    "        self.sigma = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        all_clicks = 0\n",
    "        all_satisfied = 0\n",
    "        for click_id, queries in total_clicks.items():\n",
    "            for query_id, count in queries.items():\n",
    "                self.sigma[click_id][query_id] = satisfied_clicks[click_id][query_id] / count\n",
    "                all_clicks += count\n",
    "                all_satisfied += satisfied_clicks[click_id][query_id]\n",
    "        \n",
    "        self.sigma_global = all_satisfied / all_clicks\n",
    "                \n",
    "    def _estimate_alpha(self, yandex_data):\n",
    "        seen = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        for session in yandex_data.sessions:\n",
    "            for interaction in session.get_complete_interactions():\n",
    "                query_action = interaction[0]\n",
    "                assert isinstance(query_action, QueryAction)\n",
    "                \n",
    "                for result in query_action.results:\n",
    "                    seen[result][query_action.query_id] += 1\n",
    "                \n",
    "                for click in interaction[1:]:\n",
    "                    # sanity check TODO remove\n",
    "                    assert isinstance(click, ClickAction)\n",
    "                    clicks[click.clicked_id][query_action.query_id] += 1\n",
    "                \n",
    "\n",
    "        \n",
    "        self.alpha = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        for click_id, queries in seen.items():\n",
    "            for query_id, count in queries.items():\n",
    "                self.alpha[click_id][query_id] = clicks[click_id][query_id] / count\n",
    "    \n",
    "    \n",
    "    def estimate(self, yandex_data):\n",
    "        self._estimate_sigma(yandex_data)\n",
    "        self._estimate_alpha(yandex_data)\n",
    "        \n",
    "    @staticmethod\n",
    "    def simulate(results, grade_to_probability, params):\n",
    "        clicks = []\n",
    "        for index, result in enumerate(results):\n",
    "            examine = toss_coin(grade_to_probability[result])\n",
    "            if not examine:\n",
    "                continue\n",
    "            clicks.append(index)\n",
    "            \n",
    "            satisfied = toss_coin(grade_to_probability[result])\n",
    "            if satisfied:\n",
    "                break\n",
    "                \n",
    "        return clicks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5121189928157913"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TODO compute one sigma from all of it\n",
    "    \n",
    "# TODO replace with how you compute alpha\n",
    "grade_to_probability = {\n",
    "    Grade.N: 0.1,\n",
    "    Grade.R: 0.5,\n",
    "    Grade.HR: 0.9\n",
    "}\n",
    "    \n",
    "sdbn = SimpleDynamicBayesianNetwork()\n",
    "sdbn.estimate(yandex_data)\n",
    "sdbn.sigma_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(interleaving, click_model, params):\n",
    "    winner_E = 0\n",
    "    ties = 0\n",
    "    for production, experimental in permutations:\n",
    "        interleaved = interleaving.interleave(production, experimental, \"P\", \"E\")\n",
    "        clicks = click_model.simulate([i.result for i in interleaved], grade_to_probability, params)\n",
    "        scores = interleaving.score_clicks(interleaved, clicks)\n",
    "        \n",
    "        if scores[\"E\"] > scores[\"P\"]:\n",
    "            winner_E += 1\n",
    "        elif scores[\"E\"] == scores[\"P\"]:\n",
    "            ties += 1\n",
    "\n",
    "    return (winner_E / (len(permutations) - ties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27954"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO permutations Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49633796214917675"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, RandomClickModel, {\"rho\": rcm.rho})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224135143873262"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49894859813084114"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(BalancedInterleaving, RandomClickModel, {\"rho\": rcm.rho})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simulate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9ae6c39386e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTeamDraftInterleaving\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleDynamicBayesianNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'simulate' is not defined"
     ]
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t-test, used central limit theorem. Create 30 samples by simulating 30 times for each condition \n",
    "# and compare. Since the data is parametric we can use t-tests\n",
    "\n",
    "sim_teamdraft_RCM = []\n",
    "sim_teamdraft_SDBM = []\n",
    "sim_interleaved_RCM = []\n",
    "sim_interleaved_SDBM = []\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    sim_teamdraft_RCM.append(simulate(TeamDraftInterleaving, RandomClickModel, {\"rho\": rcm.rho}))\n",
    "    sim_teamdraft_SDBM.append(simulate(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {}))\n",
    "    sim_interleaved_RCM.append(simulate(BalancedInterleaving, RandomClickModel, {\"rho\": rcm.rho}))\n",
    "    sim_interleaved_SDBM.append(simulate(BalancedInterleaving, SimpleDynamicBayesianNetwork, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the sign test (uses data of 1 trial only since non-parametric)\n",
    "\n",
    "def sign_test(interleaving, click_model, params):\n",
    "\n",
    "    sign_array = []\n",
    "    for production, experimental in permutations:\n",
    "        interleaved = interleaving.interleave(production, experimental, \"P\", \"E\")\n",
    "        clicks = click_model.simulate([i.result for i in interleaved], grade_to_probability, params)\n",
    "        scores = interleaving.score_clicks(interleaved, clicks)\n",
    "        sign_array.append(scores[\"E\"] - scores[\"P\"])\n",
    "    z, p = stats.wilcoxon(sign_array)\n",
    "    return z, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compares using t-test 2 sample (comparing with RCM, which is equal to chance)\n",
    "\n",
    "z, p = stats.ttest_ind(sim_teamdraft_RCM,sim_teamdraft_SDBM)\n",
    "print(\"P-value = {}\".format(p))\n",
    "\n",
    "z, p = stats.ttest_ind(sim_interleaved_RCM,sim_interleaved_SDBM)\n",
    "print(\"P-value = {}\".format(p))\n",
    "\n",
    "# using a one sample t-test (comparing with 0.5 i.e. chance)\n",
    "\n",
    "z, p = stats.ttest_1samp(sim_interleaved_SDBM, 0.5)\n",
    "print(\"P-value = {}\".format(p))\n",
    "\n",
    "z, p = stats.ttest_1samp(sim_interleaved_SDBM, 0.5)\n",
    "print(\"P-value = {}\".format(p))\n",
    "\n",
    "\n",
    "# using the non-parametric sign tests to compare.\n",
    "\n",
    "z, p = sign_test(TeamDraftInterleaving, RandomClickModel, {\"rho\": rcm.rho})\n",
    "print(\"P-value = {}\".format(p))\n",
    "\n",
    "z, p = sign_test(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {})\n",
    "print(\"P-value = {}\".format(p))\n",
    "\n",
    "z, p = sign_test(BalancedInterleaving, RandomClickModel, {\"rho\": rcm.rho})\n",
    "print(\"P-value = {}\".format(p))\n",
    "\n",
    "z, p = sign_test(BalancedInterleaving, SimpleDynamicBayesianNetwork, {})\n",
    "print(\"P-value = {}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part\n",
    "\n",
    "## 1.a\n",
    "\n",
    "Type I error occurs when a hypothesis test rejects $H_0$ indicating significance, where in reality this should not be the case. The probability of Type I error is as follows:\n",
    "P($H_0$ is rejected | $H_0$ is true) = $\\alpha$\n",
    "\n",
    "Hence, in order to obtain the probability asked in this question, we multiply the probability of not making a Type I error in the first m-1 cases and making this error at the $m^{th}$ case.\n",
    "\n",
    "P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$) = $(1-\\alpha)^{m-1}\\alpha$\n",
    "\n",
    "## 1.b\n",
    "We calculate the probability of at least one significant result in the presence of experiments that cannot reject the null hypothesis as indicated below, subtracting the probability of not making a Type I error at all from 1.\n",
    "\n",
    "P(at least one significant result | m experiments lacking power to reject $H_0$) = 1 - $(1-\\alpha)^m$\n",
    "\n",
    "\n",
    "\n",
    "## 2\n",
    "Both balanced interleaving and team-draft interleaving do not take the similarity of the lists into consideration. In cases where the ranks of the documents retrieved by two different search algorithms are similar, the comparison based on clicks given interleaved lists might be biased [Hofmann et al. 2011, 2013]. If the documents are swapped or their ranks are shifted by one level, team-draft interleaving might fail to capture the difference between the two algorithms. \n",
    "\n",
    "Similar to the examples given in Hofmann et al. [2011, 2013], we can consider 2 lists $l_1$ and $l_2$, both retrieving $d_1$, $d_2$ and $d_3$ in similar rankings such as: $l_1 = [d_1,d_2,d_3]$ and $l_2 = [d_2,d_3,d_1]$. If we take $d_3$ as the relevant document and assume that the one and only click occur on $d_3$, we expect that the score of $l_2$ output by the interleaving function should be higher than $l_1$'s score. The reason for this is that $l_2$ places the relevant document in the higher levels. \n",
    "\n",
    "When we construct possible interleaved lists, we observe that $d_3$ is placed at the 3rd rank in all of these lists.\n",
    "\n",
    "<img src=\"interleaving.png\">\n",
    "\n",
    "When the clicks are on $d_3$, both lists will acquire the same score of 2. This will result in a tie and the actual difference between the qualities of the algorithms will be overlooked ($l_2$ being better than $l_1$).\n",
    "\n",
    "In addition, ambiguous queries can lead to biased outcomes in team-draft interleaving where the rankings are shifted by one as indicated in [Chapelle et al. 2012].\n",
    "\n",
    "Chapelle, O., Joachims, T., Radlinski, F., and Yue,Y. Large scale validation and analysis of interleaved search evaluation. ACM Transactions on Information Science, 30(1), 2012.\n",
    "\n",
    "Hofmann, K., Whiteson, S., and de Rijke, M. A probabilistic method for inferring preferences from clicks. In\n",
    "CIKM â€™11, pp. 249â€“258, USA, 2011. ACM.\n",
    "\n",
    "Hofmann, Katja, Whiteson, Shimon, and de Rijke, Maarten. Fidelity, soundness, and efficiency of interleaved\n",
    "comparison methods. ACM Transactions on Information Systems, 31(4), 2013.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
