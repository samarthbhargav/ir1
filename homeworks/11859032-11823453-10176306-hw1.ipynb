{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import enum\n",
    "import itertools\n",
    "import operator\n",
    "import collections\n",
    "import math\n",
    "import codecs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain why we chose HR=2, R=1, N=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum.unique\n",
    "class Grade(enum.IntEnum):\n",
    "    \"\"\"\n",
    "    Represents relevance on a graded scale. \n",
    "    Any positive number represents relevance, with higher numbers representing higher relevance (total ordering)\n",
    "    Zero represents irrelevant\n",
    "    \"\"\"\n",
    "    N = (0)   # Not relevant\n",
    "    R = (1)   # Relevant\n",
    "    HR = (2)  # Highly relevant\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def is_relevant(self):\n",
    "        \"\"\"\n",
    "        'Binarizes' the relevance. Useful for computing binary metrics like precision / recall\n",
    "        \"\"\"\n",
    "        return False if self.value == 0 else True\n",
    "    \n",
    "    @classmethod\n",
    "    def from_int_list(cls, int_list):\n",
    "        \"\"\"\n",
    "        Converts a list of integers to a list of Grade\n",
    "        \"\"\"\n",
    "        members = dict((member.value, member) for member in cls.__members__.values())\n",
    "        return [members[value] for value in int_list]\n",
    "\n",
    "    @classmethod\n",
    "    def max_grade(cls):\n",
    "        return max(member.value for member in cls.__members__.values())\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "\n",
    "P: {N N N N N}\n",
    "E: {N N N N R}\n",
    "\n",
    "â€¦\n",
    "P: {HR HR HR HR R}\n",
    "E: {HR HR HR HR HR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_permutations(grade, sequence_length):\n",
    "    \"\"\"\n",
    "    For a given grade and a sequence length n, returns all possible permutations of length n that can be formed. \n",
    "    Note that a generator is returned and not a list\n",
    "    \"\"\"\n",
    "    for i, perm in enumerate(itertools.product(range(len(grade.__members__)), repeat=sequence_length)):\n",
    "        yield grade.from_int_list(perm)\n",
    "\n",
    "def get_all_permutation_pairs(grade, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates all possible pairs of permutations given a grade and a sequence length.\n",
    "    Note that a generator is returned and not a list\n",
    "    \"\"\"\n",
    "    sequence_1 = get_all_permutations(grade, sequence_length)\n",
    "    sequence_2 = get_all_permutations(grade, sequence_length)    \n",
    "    for pair in itertools.product(sequence_1, sequence_2):\n",
    "        yield pair\n",
    "        \n",
    "def count_sequence(seq):\n",
    "    \"\"\"\n",
    "    `len` for generators\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for _ in seq:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# A sanity check to ensure the correct number of sequences are being generated\n",
    "# the `-(3**5)` removes exact duplicates\n",
    "assert count_sequence(get_all_permutation_pairs(Grade, 5)) == (3**5) * (3**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO add assumption that we weed out exactly similar documents (or do WE?) dun dun dun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = np.array(list(get_all_permutation_pairs(Grade, 5)), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[N, N, N, N, N],\n",
       "        [N, N, N, N, N]],\n",
       "\n",
       "       [[N, N, N, N, N],\n",
       "        [N, N, N, N, R]],\n",
       "\n",
       "       [[N, N, N, N, N],\n",
       "        [N, N, N, N, HR]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, N]],\n",
       "\n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, R]],\n",
       "\n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, HR]]], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[N, N, N, N, N],\n",
       "       [N, N, N, N, N]], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[HR, HR, HR, HR, HR],\n",
       "       [HR, HR, HR, HR, HR]], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "production = np.array(list(map(operator.itemgetter(0), permutations)))\n",
    "experiment = np.array(list(map(operator.itemgetter(1), permutations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain this in detail. It's one of the central assumptions we make. \n",
    "Single Query Assumption.\n",
    "We can assume that the entire experiment is for a single query, so there are only 10 R + 10 HR documents i.e a total of 20 relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO double check this\n",
    "relevant_count_production = 20\n",
    "relevant_count_experiment = 20\n",
    "(relevant_count_experiment, relevant_count_production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Evaluation Measures (10 points)\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.   ... 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "class Metric:\n",
    "    \n",
    "    \"\"\"\n",
    "    Common parent for all metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def name(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        \"\"\" TODO \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Precision(Metric):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"precision\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(results))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        # binarize, compute how many are relevant and divide by k\n",
    "        return sum(r.is_relevant for r in results) / k\n",
    "    \n",
    "class Recall(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"recall\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"relevant_count\" in kwargs\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(production))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        # binarize and count, divide by number of relevant documents\n",
    "        # the number of relevant documents is a function of the total number of relevant documents\n",
    "        # and how many *can* be present in a list of length k\n",
    "        # if there are only k = 5 and total = 20, recall can never be 1 even when the all documents are relevant\n",
    "        # therefore, the denom should be min(k, total)\n",
    "        # TODO verify\n",
    "        return sum(r.is_relevant for r in results) / kwargs[\"relevant_count\"] \n",
    "        \n",
    "    \n",
    "class AveragePrecision(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"relevant_count\" in kwargs\n",
    "        P = Precision()\n",
    "        precisions = np.zeros(len(results))\n",
    "        for k in range(1, len(results) + 1):\n",
    "            precisions[k - 1] = P.compute(results, k = k)\n",
    "        # TODO verify\n",
    "        return precisions.sum() / kwargs[\"relevant_count\"] \n",
    "\n",
    "AP = AveragePrecision()\n",
    "ap = np.array([AP.compute(r, relevant_count = relevant_count_production) for r in production])\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Explain why the max in our case can only be 25, so this shit is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1215.,  2430.,  4374.,  2916.,  4374.,  8748.,  2916.,  8748.,\n",
       "         7776., 15552.]),\n",
       " array([0.   , 0.025, 0.05 , 0.075, 0.1  , 0.125, 0.15 , 0.175, 0.2  ,\n",
       "        0.225, 0.25 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAFTtJREFUeJzt3X+s3fV93/Hna/YgTbcGA3css53YWZxMJmoVckPYonZNqMAkWYw0GoG64qVWLTVO262VEmikIZEgEa0qC1pC5AYvpsriMJYVayFlHiGNJoUfl0AgQCk3hgRbEG6wIWtpyUze++N8SA/3e6/v5Zzje2zf50M6ut/v+/v5fr+fzz1Xfvn745xvqgpJkvr9vXF3QJJ07DEcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSepYOe4ODOr000+vdevWjbsbknRcueeee35YVRMLtVswHJLsBN4HPF1Vb+mr/zawHXgR+EpVfaTVLwe2tvrvVNWtrb4J+BSwAvhcVV3d6uuB3cBpwD3Ar1fVjxfq17p165iamlqomSSpT5LvLabdYk4rfR7YNGvj7wI2A79QVWcCf9jqG4GLgTPbOp9JsiLJCuDTwAXARuCS1hbgk8A1VfVG4BC9YJEkjdGC4VBV3wAOzir/FnB1Vb3Q2jzd6puB3VX1QlU9BkwDZ7fXdFXta0cFu4HNSQK8G7iprb8LuHDIMUmShjToBek3Ab+Y5M4kf57k7a2+Gniir93+VpuvfhrwbFUdnlWXJI3RoBekVwKnAucAbwduTPKGkfVqHkm2AdsAXve61x3t3UnSsjXokcN+4MvVcxfwE+B04ACwtq/dmlabr/4McEqSlbPqc6qqHVU1WVWTExMLXmyXJA1o0HD4U+BdAEneBJwE/BDYA1yc5OR2F9IG4C7gbmBDkvVJTqJ30XpP9Z40dDtwUdvuFuDmQQcjSRqNxdzK+kXgl4HTk+wHrgB2AjuTfAf4MbCl/UP/YJIbgYeAw8D2qnqxbefDwK30bmXdWVUPtl18FNid5BPAvcD1IxyfJGkAOV4fEzo5OVl+zkGSXpkk91TV5ELt/PoMSVLHcfv1GZI0Tusu+8pY9vv41e9dkv145CBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUseC4ZBkZ5Kn2/OiZy/7/SSV5PQ2nyTXJplOcn+Ss/rabknyaHtt6au/LckDbZ1rk2RUg5MkDWYxRw6fBzbNLiZZC5wHfL+vfAGwob22Ade1tqcCVwDvAM4Grkiyqq1zHfCbfet19iVJWloLhkNVfQM4OMeia4CPANVX2wzcUD13AKckeS1wPrC3qg5W1SFgL7CpLfu5qrqjqgq4AbhwuCFJkoY10DWHJJuBA1X17VmLVgNP9M3vb7Uj1ffPUZ9vv9uSTCWZmpmZGaTrkqRFeMXhkOTVwB8A/2H03TmyqtpRVZNVNTkxMbHUu5ekZWOQI4d/CqwHvp3kcWAN8K0k/xg4AKzta7um1Y5UXzNHXZI0Rq84HKrqgar6R1W1rqrW0TsVdFZVPQXsAS5tdy2dAzxXVU8CtwLnJVnVLkSfB9zalv0oyTntLqVLgZtHNDZJ0oAWcyvrF4FvAm9Osj/J1iM0vwXYB0wDfwx8CKCqDgIfB+5urytbjdbmc22d7wJfHWwokqRRWblQg6q6ZIHl6/qmC9g+T7udwM456lPAWxbqhyRp6fgJaUlSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVLHYh4TujPJ00m+01f7j0n+Isn9Sf5HklP6ll2eZDrJI0nO76tvarXpJJf11dcnubPVv5TkpFEOUJL0yi3myOHzwKZZtb3AW6rq54G/BC4HSLIRuBg4s63zmSQrkqwAPg1cAGwELmltAT4JXFNVbwQOAUd6RrUkaQksGA5V9Q3g4Kza/6qqw232DmBNm94M7K6qF6rqMWAaOLu9pqtqX1X9GNgNbE4S4N3ATW39XcCFQ45JkjSkUVxz+A3gq216NfBE37L9rTZf/TTg2b6geakuSRqjocIhyceAw8AXRtOdBfe3LclUkqmZmZml2KUkLUsDh0OSfwu8D/i1qqpWPgCs7Wu2ptXmqz8DnJJk5az6nKpqR1VNVtXkxMTEoF2XJC1goHBIsgn4CPD+qnq+b9Ee4OIkJydZD2wA7gLuBja0O5NOonfRek8LlduBi9r6W4CbBxuKJGlUFnMr6xeBbwJvTrI/yVbgPwP/ENib5L4knwWoqgeBG4GHgD8DtlfVi+2awoeBW4GHgRtbW4CPAr+XZJreNYjrRzpCSdIrtnKhBlV1yRzlef8Br6qrgKvmqN8C3DJHfR+9u5kkSccIPyEtSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdi3mG9M4kTyf5Tl/t1CR7kzzafq5q9SS5Nsl0kvuTnNW3zpbW/tEkW/rqb0vyQFvn2iQZ9SAlSa/MYo4cPg9smlW7DLitqjYAt7V5gAuADe21DbgOemECXAG8g97zoq94KVBam9/sW2/2viRJS2zBcKiqbwAHZ5U3A7va9C7gwr76DdVzB3BKktcC5wN7q+pgVR0C9gKb2rKfq6o7qqqAG/q2JUkak0GvOZxRVU+26aeAM9r0auCJvnb7W+1I9f1z1CVJYzT0Ben2P/4aQV8WlGRbkqkkUzMzM0uxS0lalgYNhx+0U0K0n0+3+gFgbV+7Na12pPqaOepzqqodVTVZVZMTExMDdl2StJBBw2EP8NIdR1uAm/vql7a7ls4Bnmunn24Fzkuyql2IPg+4tS37UZJz2l1Kl/ZtS5I0JisXapDki8AvA6cn2U/vrqOrgRuTbAW+B3ygNb8FeA8wDTwPfBCgqg4m+Thwd2t3ZVW9dJH7Q/TuiPoZ4KvtJUkaowXDoaoumWfRuXO0LWD7PNvZCeycoz4FvGWhfkiSlo6fkJYkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktSx4HcrSXpl1l32lbHt+/Gr3zuW/Y5rzOMa73LgkYMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSx1DhkOTfJ3kwyXeSfDHJq5KsT3JnkukkX0pyUmt7cpufbsvX9W3n8lZ/JMn5ww1JkjSsgcMhyWrgd4DJqnoLsAK4GPgkcE1VvRE4BGxtq2wFDrX6Na0dSTa29c4ENgGfSbJi0H5JkoY37GmllcDPJFkJvBp4Eng3cFNbvgu4sE1vbvO05ecmSavvrqoXquoxYBo4e8h+SZKGMHA4VNUB4A+B79MLheeAe4Bnq+pwa7YfWN2mVwNPtHUPt/an9dfnWOdlkmxLMpVkamZmZtCuS5IWMMxppVX0/te/HvgnwM/SOy101FTVjqqarKrJiYmJo7krSVrWhjmt9CvAY1U1U1X/D/gy8E7glHaaCWANcKBNHwDWArTlrwGe6a/PsY4kaQyG+eK97wPnJHk18DfAucAUcDtwEbAb2ALc3NrvafPfbMu/VlWVZA/wX5P8Eb0jkA3AXUP0S9IyMc4vOTzRDRwOVXVnkpuAbwGHgXuBHcBXgN1JPtFq17dVrgf+JMk0cJDeHUpU1YNJbgQeatvZXlUvDtovSdLwhvrK7qq6ArhiVnkfc9xtVFV/C/zqPNu5CrhqmL5IkkbHT0hLkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOoYKhySnJLkpyV8keTjJP09yapK9SR5tP1e1tklybZLpJPcnOatvO1ta+0eTbBl2UJKk4Qx75PAp4M+q6p8BvwA8DFwG3FZVG4Db2jzABcCG9toGXAeQ5FR6jxp9B73Hi17xUqBIksZj4HBI8hrgl4DrAarqx1X1LLAZ2NWa7QIubNObgRuq5w7glCSvBc4H9lbVwao6BOwFNg3aL0nS8IY5clgPzAD/Jcm9ST6X5GeBM6rqydbmKeCMNr0aeKJv/f2tNl9dkjQmw4TDSuAs4Lqqeivw1/zdKSQAqqqAGmIfL5NkW5KpJFMzMzOj2qwkaZZhwmE/sL+q7mzzN9ELix+000W0n0+35QeAtX3rr2m1+eodVbWjqiaranJiYmKIrkuSjmTgcKiqp4Ankry5lc4FHgL2AC/dcbQFuLlN7wEubXctnQM8104/3Qqcl2RVuxB9XqtJksZk5ZDr/zbwhSQnAfuAD9ILnBuTbAW+B3ygtb0FeA8wDTzf2lJVB5N8HLi7tbuyqg4O2S9J0hCGCoequg+YnGPRuXO0LWD7PNvZCewcpi+SpNHxE9KSpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkjmEfE6rjxLrLvjKW/T5+9XvHsl8Y35ilE8HQRw5JViS5N8n/bPPrk9yZZDrJl9rzpUlycpufbsvX9W3j8lZ/JMn5w/ZJkjScUZxW+l3g4b75TwLXVNUbgUPA1lbfChxq9WtaO5JsBC4GzgQ2AZ9JsmIE/ZIkDWiocEiyBngv8Lk2H+DdwE2tyS7gwja9uc3Tlp/b2m8GdlfVC1X1GDANnD1MvyRJwxn2yOE/AR8BftLmTwOerarDbX4/sLpNrwaeAGjLn2vtf1qfYx1J0hgMHA5J3gc8XVX3jLA/C+1zW5KpJFMzMzNLtVtJWnaGOXJ4J/D+JI8Du+mdTvoUcEqSl+6CWgMcaNMHgLUAbflrgGf663Os8zJVtaOqJqtqcmJiYoiuS5KOZOBwqKrLq2pNVa2jd0H5a1X1a8DtwEWt2Rbg5ja9p83Tln+tqqrVL253M60HNgB3DdovSdLwjsbnHD4K7E7yCeBe4PpWvx74kyTTwEF6gUJVPZjkRuAh4DCwvapePAr9kiQt0kjCoaq+Dny9Te9jjruNqupvgV+dZ/2rgKtG0RdJ0vD8+gxJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKHjwnVUeWjOpeWv2+NikcOkqQOw0GS1GE4SJI6DAdJUofhIEnq8G6lJeSdJJKOFx45SJI6DAdJUsfA4ZBkbZLbkzyU5MEkv9vqpybZm+TR9nNVqyfJtUmmk9yf5Ky+bW1p7R9NsmX4YUmShjHMkcNh4PeraiNwDrA9yUbgMuC2qtoA3NbmAS4ANrTXNuA66IUJcAXwDnrPnr7ipUCRJI3HwOFQVU9W1bfa9P8FHgZWA5uBXa3ZLuDCNr0ZuKF67gBOSfJa4Hxgb1UdrKpDwF5g06D9kiQNbyTXHJKsA94K3AmcUVVPtkVPAWe06dXAE32r7W+1+epz7WdbkqkkUzMzM6PouiRpDkOHQ5J/APx34N9V1Y/6l1VVATXsPvq2t6OqJqtqcmJiYlSblSTNMlQ4JPn79ILhC1X15Vb+QTtdRPv5dKsfANb2rb6m1earS5LGZJi7lQJcDzxcVX/Ut2gP8NIdR1uAm/vql7a7ls4Bnmunn24Fzkuyql2IPq/VJEljMswnpN8J/DrwQJL7Wu0PgKuBG5NsBb4HfKAtuwV4DzANPA98EKCqDib5OHB3a3dlVR0col+SpCENHA5V9X+AzLP43DnaF7B9nm3tBHYO2hdJ0mj5CWlJUofhIEnqMBwkSR2GgySpw3CQJHUsy4f9+NAdSToyjxwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1HDPhkGRTkkeSTCe5bNz9kaTl7JgIhyQrgE8DFwAbgUuSbBxvryRp+TomwgE4G5iuqn1V9WNgN7B5zH2SpGXrWAmH1cATffP7W02SNAbH1fMckmwDtrXZv0ryyICbOh344Wh6ddxwzMvDchvzchsv+eTQY379YhodK+FwAFjbN7+m1V6mqnYAO4bdWZKpqpocdjvHE8e8PCy3MS+38cLSjflYOa10N7AhyfokJwEXA3vG3CdJWraOiSOHqjqc5MPArcAKYGdVPTjmbknSsnVMhANAVd0C3LJEuxv61NRxyDEvD8ttzMttvLBEY05VLcV+JEnHkWPlmoMk6RhywoXDQl/DkeTkJF9qy+9Msq5v2eWt/kiS85ey34MadLxJ1iX5myT3tddnl7rvg1rEmH8pybeSHE5y0axlW5I82l5blq7XwxlyzC/2vc/HzY0eixjz7yV5KMn9SW5L8vq+ZSfq+3ykMY/2fa6qE+ZF72L2d4E3ACcB3wY2zmrzIeCzbfpi4EttemNrfzKwvm1nxbjHdBTHuw74zrjHcJTGvA74eeAG4KK++qnAvvZzVZteNe4xHc0xt2V/Ne4xHKUxvwt4dZv+rb6/7RP5fZ5zzEfjfT7RjhwW8zUcm4Fdbfom4NwkafXdVfVCVT0GTLftHcuGGe/xasExV9XjVXU/8JNZ654P7K2qg1V1CNgLbFqKTg9pmDEfrxYz5tur6vk2ewe9z0fBif0+zzfmkTvRwmExX8Px0zZVdRh4Djhtkesea4YZL8D6JPcm+fMkv3i0Ozsiw7xPx+N7DMP3+1VJppLckeTC0XbtqHmlY94KfHXAdY8Vw4wZRvw+HzO3smrJPQm8rqqeSfI24E+TnFlVPxp3xzRyr6+qA0neAHwtyQNV9d1xd2pUkvwbYBL4l+Puy1KZZ8wjfZ9PtCOHxXwNx0/bJFkJvAZ4ZpHrHmsGHm87ffYMQFXdQ+9c55uOeo+HN8z7dDy+xzBkv6vqQPu5D/g68NZRdu4oWdSYk/wK8DHg/VX1witZ9xg0zJhH/z6P+yLMiC/orKR38Wk9f3dB58xZbbbz8gu0N7bpM3n5Bel9HPsXpIcZ78RL46N3AewAcOq4xzSKMfe1/TzdC9KP0btIuapNn+hjXgWc3KZPBx5l1kXOY/G1yL/tt9L7T82GWfUT9n0+wphH/j6P/RdyFH7B7wH+sv0CP9ZqV9JLWYBXAf+N3gXnu4A39K37sbbeI8AF4x7L0Rwv8K+BB4H7gG8B/2rcYxnhmN9O73ztX9M7Knywb93faL+LaeCD4x7L0R4z8C+AB9o/NA8AW8c9lhGO+X8DP2h/w/cBe5bB+zznmI/G++wnpCVJHSfaNQdJ0ggYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqeP/A96rkJuYl5NJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41a57727b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HR, HR, HR, HR, HR, HR, HR, HR, HR, HR, R, R, R, R, R, R, R, R, R, R, N, N, N, N, N, N, N, N, N, N]\n"
     ]
    }
   ],
   "source": [
    "class DCG(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        \n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(production))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        \n",
    "        dcg = 0.0 \n",
    "        for rank, result in enumerate(results):\n",
    "            rank += 1 # TODO verify\n",
    "            gain = 2**result.value - 1\n",
    "            discount = math.log2(1 + rank)\n",
    "            dcg += gain / discount\n",
    "        return dcg\n",
    "\n",
    "\n",
    "class NDCG(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "\n",
    "    # NDCG: We considered two possibilities. TODO explain it\n",
    "    # Normalization is done according to the slides mentioned here \n",
    "    # https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"ideal_ranking\" in kwargs\n",
    "        \n",
    "        dcg = DCG()\n",
    "        \n",
    "        ideal_ranking = kwargs[\"ideal_ranking\"]\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(results))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        ideal_ranking = ideal_ranking[:k]\n",
    "        \n",
    "        max_dcg = dcg.compute(ideal_ranking)\n",
    "        \n",
    "        return NDCG.dcg.compute(results) / max_dcg\n",
    "\n",
    "ideal_ranking = [Grade.HR] * 10 + [Grade.R] * 10 + [Grade.N] * 10\n",
    "print(ideal_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623046875"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ERR(Metric):\n",
    "    # TODO cite this http://delivery.acm.org/10.1145/1650000/1646033/p621-chapelle.pdf?ip=145.109.35.88&id=1646033&acc=ACTIVE%20SERVICE&key=0C390721DC3021FF%2E86041C471C98F6DA%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=1026312371&CFTOKEN=18042814&__acm__=1515748872_543fbe0fe6a5c9d1fadb8f8341c9da3c\n",
    "    # TODO and explain how you converted to code\n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"grade\" in kwargs\n",
    "        grade = kwargs[\"grade\"]\n",
    "        \n",
    "        def r_func(result):\n",
    "            return ((2 ** result.value) - 1) / (2 ** grade.max_grade())  \n",
    "        \n",
    "        err = 0.0\n",
    "        for rank, result in enumerate(results):\n",
    "            previous_results = results[:rank]\n",
    "            satisfaction = 1\n",
    "            for previous_result in previous_results:\n",
    "                satisfaction = satisfaction * ((1 - r_func(previous_result)))\n",
    "            # rank is 0-indexed here, so add 1\n",
    "            # I am not sure, but I think r_func(result) should be here. Please check TODO\n",
    "            err += (1/(rank + 1)) * satisfaction * r_func(result)\n",
    "        return err\n",
    "# TODO this is incorrect. I couldn't figure out what is going wrong\n",
    "err = ERR()\n",
    "err.compute([Grade.HR, Grade.HR, Grade.HR, Grade.HR], grade=Grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the ð›¥measure (0 points)\n",
    "\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ð›¥measure = measureE-measureP. Consider only those pairs for which E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27962"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision = AveragePrecision()\n",
    "ndcg = NDCG() # TODO or pick ERR?\n",
    "\n",
    "# TODO which measure do we use to subset? All 3 or just one? right now I\"m doing only one\n",
    "\n",
    "average_precision_P = np.array([average_precision.compute(results, relevant_count = relevant_count_production) \\\n",
    "                                for results in production])\n",
    "average_precision_E = np.array([average_precision.compute(results, relevant_count = relevant_count_experiment) \\\n",
    "                                for results in experiment])\n",
    "delta_measure = average_precision_E - average_precision_P\n",
    "len(permutations[delta_measure > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = permutations[delta_measure > 0]\n",
    "production = production[delta_measure > 0]\n",
    "experiment = experiment[delta_measure > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Interleaving (15 points)\n",
    "Implement 2 interleaving algorithms: \n",
    "(1) Team-Draft Interleaving OR Balanced Interleaving, ~~AND (2), Probabilistic Interleaving.~~ \n",
    "\n",
    "The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toss_coin(heads = 0.5):\n",
    "    # head = True, Tails is False\n",
    "    return True if random.random() < heads else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interleaved(source='B', source_index=0, result=R),\n",
       " Interleaved(source='A', source_index=0, result=R),\n",
       " Interleaved(source='B', source_index=1, result=N),\n",
       " Interleaved(source='A', source_index=1, result=HR),\n",
       " Interleaved(source='B', source_index=2, result=HR),\n",
       " Interleaved(source='A', source_index=2, result=N)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Interleaved = collections.namedtuple(\"Interleaved\", [\"source\", \"source_index\", \"result\"])\n",
    "        \n",
    "class BalancedInterleaving:\n",
    "    \"\"\" TODO\"\"\"\n",
    "    # TODO Ece, please clean up comments if you don't think they're necessary\n",
    "    \n",
    "    @staticmethod\n",
    "    def interleave(A, B, A_name, B_name):\n",
    "\n",
    "        selected = toss_coin()\n",
    "        interleaved = []\n",
    "        \n",
    "        ka = 0\n",
    "        kb = 0\n",
    "        \n",
    "        while(ka <= len(A) and kb <= len(B) and (ka < len(A) or kb < len(B))):\n",
    "\n",
    "            if (ka < kb) or (ka==kb and selected):\n",
    "                    tup = Interleaved(A_name, ka, A[ka])\n",
    "                    # we don't need to do that - explain why according to our assumptions TODO\n",
    "#                     if(tup not in interleaved):\n",
    "#                         interleaved.append(tup)\n",
    "                    interleaved.append(tup)\n",
    "                    ka += 1\n",
    "\n",
    "            else:  \n",
    "                    tup = Interleaved(B_name, kb, B[kb])\n",
    "                    interleaved.append(tup)\n",
    "                    kb += 1\n",
    "        \n",
    "        return interleaved\n",
    "    \n",
    "    @staticmethod\n",
    "    def score_clicks(interleaved, clicks):\n",
    "        scores = collections.defaultdict(int)\n",
    "        \n",
    "#         #assumption: user reads from top to bottom\n",
    "#         cmax = max(clicks) #ranking of the lowest click\n",
    "\n",
    "#         #assumption: all docs are unique\n",
    "#         #they only come from one engine, no need to check their min index given 2 engine rankings\n",
    "#         #also no doc attributed to both engines\n",
    "#         engine, k, relevance = interleaved[cmax]\n",
    "\n",
    "        for c in clicks:\n",
    "            il = interleaved[c]\n",
    "            scores[il.source] += 1\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "A = [Grade.R, Grade.HR, Grade.N]\n",
    "B = [Grade.R, Grade.N, Grade.HR]\n",
    "\n",
    "interleaved = BalancedInterleaving.interleave(A, B, \"A\", \"B\")\n",
    "interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'B': 3})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks = [0, 2, 4]\n",
    "BalancedInterleaving.score_clicks(interleaved, clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interleaved(source='A', source_index=0, result=R),\n",
       " Interleaved(source='B', source_index=0, result=R),\n",
       " Interleaved(source='A', source_index=1, result=HR),\n",
       " Interleaved(source='A', source_index=2, result=N),\n",
       " Interleaved(source='B', source_index=1, result=N),\n",
       " Interleaved(source='B', source_index=2, result=HR)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TeamDraftInterleaving:\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def interleave(A, B, A_name, B_name):\n",
    "\n",
    "        interleaved = []\n",
    "        \n",
    "        ka = 0\n",
    "        kb = 0\n",
    "        \n",
    "        \n",
    "        while ka < len(A) and kb < len(B):\n",
    "            selected = toss_coin()\n",
    "            if selected:\n",
    "                tup = Interleaved(A_name, ka, A[ka])\n",
    "                interleaved.append(tup)\n",
    "                ka += 1\n",
    "            else:\n",
    "                tup = Interleaved(B_name, kb, B[kb])\n",
    "                interleaved.append(tup)\n",
    "                kb += 1\n",
    "        \n",
    "        # add the remaining results\n",
    "        if ka < len(A):\n",
    "            while ka < len(A):\n",
    "                tup = Interleaved(A_name, ka, A[ka])\n",
    "                interleaved.append(tup)\n",
    "                ka += 1\n",
    "        elif kb < len(B):\n",
    "            while kb < len(B):\n",
    "                tup = Interleaved(B_name, kb, B[kb])\n",
    "                interleaved.append(tup)\n",
    "                kb += 1\n",
    "        \n",
    "        return interleaved\n",
    "    \n",
    "    @staticmethod\n",
    "    def score_clicks(interleaved, clicks):\n",
    "        scores = collections.defaultdict(int)\n",
    "        \n",
    "#         #assumption: user reads from top to bottom\n",
    "#         cmax = max(clicks) #ranking of the lowest click\n",
    "\n",
    "#         #assumption: all docs are unique\n",
    "#         #they only come from one engine, no need to check their min index given 2 engine rankings\n",
    "#         #also no doc attributed to both engines\n",
    "#         engine, k, relevance = interleaved[cmax]\n",
    "\n",
    "        for c in clicks:\n",
    "            il = interleaved[c]\n",
    "            scores[il.source] += 1\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "A = [Grade.R, Grade.HR, Grade.N]\n",
    "B = [Grade.R, Grade.N, Grade.HR]\n",
    "\n",
    "interleaved = TeamDraftInterleaving.interleave(A, B, \"A\", \"B\")\n",
    "interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sessions:  11717\n",
      "Total number of interactions:  100000\n"
     ]
    }
   ],
   "source": [
    "ClickAction = collections.namedtuple(\"ClickAction\", [\"clicked_id\"])\n",
    "QueryAction = collections.namedtuple(\"QueryAction\", [\"query_id\", \"results\"])\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, session_id):\n",
    "        self.session_id = session_id\n",
    "        self.data = []\n",
    "    \n",
    "    @property\n",
    "    def clicks(self):\n",
    "        return list(filter(lambda _: isinstance(_, ClickAction), self.data))\n",
    "    \n",
    "    @property\n",
    "    def queries(self):\n",
    "        return list(filter(lambda _: isinstance(_, QueryAction), self.data))\n",
    "    \n",
    "    def get_complete_interactions(self):\n",
    "        # return complete interactions\n",
    "        last_query = 0\n",
    "        for index, action in enumerate(self.data):\n",
    "            if index != 0 and isinstance(action, QueryAction):\n",
    "                yield self.data[last_query: index]\n",
    "                last_query = index\n",
    "        if isinstance(self.data[-1], ClickAction):\n",
    "            yield self.data[last_query:]\n",
    "        \n",
    "class YandexData:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.sessions = []\n",
    "        self.u = 0\n",
    "        self.q = 0\n",
    "        self._read()\n",
    "        \n",
    "    def _read(self):\n",
    "        with codecs.open(self.file_path, \"r\", \"utf-8\") as reader:\n",
    "            current_session = Session(0)\n",
    "            for line in reader:\n",
    "                line = line.split(\"\\t\")\n",
    "                session_id = int(line[0])\n",
    "                interaction_type = line[2]\n",
    "                if current_session.session_id != session_id:\n",
    "                    self.sessions.append(current_session)\n",
    "                    current_session = Session(session_id)\n",
    "                \n",
    "                if interaction_type == \"C\":\n",
    "                    current_session.data.append(ClickAction(int(line[3])))\n",
    "                else:\n",
    "                    results = [int(q_id) for q_id in line[5:]]\n",
    "                    self.u = max(self.u, max(results))\n",
    "                    query_id = int(line[3])\n",
    "                    self.q = max(self.q, query_id)\n",
    "                    current_session.data.append(QueryAction(query_id, results))\n",
    "        \n",
    "        self.sessions.append(current_session)\n",
    "\n",
    "\n",
    "yandex_data = YandexData(\"./YandexRelPredChallenge.txt\")\n",
    "print(\"Total number of sessions: \", len(yandex_data.sessions))\n",
    "print(\"Total number of interactions: \", sum(len(session.data) for session in yandex_data.sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13445559411047547"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomClickModel:\n",
    "    def __init__(self):\n",
    "        self.rho = 0\n",
    "    \n",
    "    def estimate(self, yandex_data):\n",
    "        clicks = 0\n",
    "        results = 0\n",
    "        for session in yandex_data.sessions:\n",
    "            clicks += len(session.clicks)\n",
    "            results += sum(len(query.results) for query in session.queries)\n",
    "        self.rho = clicks / results\n",
    "    \n",
    "    def predict(self, clicks):\n",
    "        #(b) there is a method that predicts the click probability given a ranked list of relevance labels,\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def simulate(results, grade_to_probability, params):\n",
    "        rho = params.get(\"rho\")\n",
    "        assert 0 <= rho <= 1.0\n",
    "        \n",
    "        # TODO something is wrong with this and I can't figure it out. pls fix \n",
    "        clicks = []\n",
    "        for rank, result in enumerate(results):\n",
    "            click = toss_coin(rho)\n",
    "            if click:\n",
    "                clicks.append(rank)\n",
    "        return clicks\n",
    "\n",
    "rcm = RandomClickModel()\n",
    "rcm.estimate(yandex_data)\n",
    "rcm.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527494, 66579)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yandex_data.u, yandex_data.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDynamicBayesianNetwork:\n",
    "        \n",
    "    def _estimate_sigma(self, yandex_data):\n",
    "        satisfied_clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        total_clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        \n",
    "        for session in yandex_data.sessions:\n",
    "            for interaction in session.get_complete_interactions():\n",
    "                #print(interaction) TODO explain why we continue\n",
    "                if not isinstance(interaction[-1], ClickAction):\n",
    "                    continue\n",
    "                query_action = interaction[0]\n",
    "                # sanity check TODO remove\n",
    "                assert isinstance(query_action, QueryAction)\n",
    "                for click in interaction[1:]:\n",
    "                    # sanity check TODO remove\n",
    "                    assert isinstance(click, ClickAction)\n",
    "                    total_clicks[click.clicked_id][query_action.query_id] += 1\n",
    "                \n",
    "                last_click = interaction[-1]\n",
    "                #print(last_click)\n",
    "                satisfied_clicks[last_click.clicked_id][query_action.query_id] += 1\n",
    "        \n",
    "        self.sigma = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        all_clicks = 0\n",
    "        all_satisfied = 0\n",
    "        for click_id, queries in total_clicks.items():\n",
    "            for query_id, count in queries.items():\n",
    "                self.sigma[click_id][query_id] = satisfied_clicks[click_id][query_id] / count\n",
    "                all_clicks += count\n",
    "                all_satisfied += satisfied_clicks[click_id][query_id]\n",
    "        \n",
    "        self.sigma_global = all_satisfied / all_clicks\n",
    "                \n",
    "    def _estimate_alpha(self, yandex_data):\n",
    "        seen = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        for session in yandex_data.sessions:\n",
    "            for interaction in session.get_complete_interactions():\n",
    "                query_action = interaction[0]\n",
    "                assert isinstance(query_action, QueryAction)\n",
    "                \n",
    "                for result in query_action.results:\n",
    "                    seen[result][query_action.query_id] += 1\n",
    "                \n",
    "                for click in interaction[1:]:\n",
    "                    # sanity check TODO remove\n",
    "                    assert isinstance(click, ClickAction)\n",
    "                    clicks[click.clicked_id][query_action.query_id] += 1\n",
    "                \n",
    "\n",
    "        \n",
    "        self.alpha = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        for click_id, queries in seen.items():\n",
    "            for query_id, count in queries.items():\n",
    "                self.alpha[click_id][query_id] = clicks[click_id][query_id] / count\n",
    "    \n",
    "    \n",
    "    def estimate(self, yandex_data):\n",
    "        self._estimate_sigma(yandex_data)\n",
    "        self._estimate_alpha(yandex_data)\n",
    "        \n",
    "    @staticmethod\n",
    "    def simulate(results, grade_to_probability, params):\n",
    "        clicks = []\n",
    "        for index, result in enumerate(results):\n",
    "            examine = toss_coin(grade_to_probability[result])\n",
    "            if not examine:\n",
    "                continue\n",
    "            clicks.append(index)\n",
    "            \n",
    "            satisfied = toss_coin(grade_to_probability[result])\n",
    "            if satisfied:\n",
    "                break\n",
    "                \n",
    "        return clicks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5121189928157913"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TODO compute one sigma from all of it\n",
    "    \n",
    "# TODO replace with how you compute alpha\n",
    "grade_to_probability = {\n",
    "    Grade.N: 0.1,\n",
    "    Grade.R: 0.5,\n",
    "    Grade.HR: 0.9\n",
    "}\n",
    "    \n",
    "sdbn = SimpleDynamicBayesianNetwork()\n",
    "sdbn.estimate(yandex_data)\n",
    "sdbn.sigma_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(interleaving, click_model, params):\n",
    "    winner_E = 0\n",
    "    for production, experimental in permutations:\n",
    "        interleaved = interleaving.interleave(production, experimental, \"P\", \"E\")\n",
    "        clicks = click_model.simulate([i.result for i in interleaved], grade_to_probability, params)\n",
    "        scores = interleaving.score_clicks(interleaved, clicks)\n",
    "        if scores[\"E\"] > scores[\"P\"]:\n",
    "            winner_E += 1\n",
    "    return (winner_E / len(permutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27962"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO permutations Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30137329232529864"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, RandomClickModel, {\"rho\": rcm.rho})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5596523853801588"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3028395679851227"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(BalancedInterleaving, RandomClickModel, {\"rho\": rcm.rho})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5601888276947286"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part\n",
    "\n",
    "## 1.a\n",
    "P($m^{th}$ experiment gives significant result | m experiments lacking power to reject H0) = $(1-\\alpha)^{m-1}\\alpha$\n",
    "\n",
    "## 1.b\n",
    "P(at least one significant result | m experiments lacking power to reject H0) = 1 - $(1-\\alpha)^m$\n",
    "\n",
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
