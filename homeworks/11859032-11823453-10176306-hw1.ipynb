{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import enum\n",
    "import itertools\n",
    "import operator\n",
    "import collections\n",
    "import math\n",
    "import codecs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain why we chose HR=2, R=1, N=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@enum.unique\n",
    "class Grade(enum.IntEnum):\n",
    "    \"\"\"\n",
    "    Represents relevance on a graded scale. \n",
    "    Any positive number represents relevance, with higher numbers representing higher relevance (total ordering)\n",
    "    Zero represents irrelevant\n",
    "    \"\"\"\n",
    "    N = (0)   # Not relevant\n",
    "    R = (1)   # Relevant\n",
    "    HR = (2)  # Highly relevant\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def is_relevant(self):\n",
    "        \"\"\"\n",
    "        'Binarizes' the relevance. Useful for computing binary metrics like precision / recall\n",
    "        \"\"\"\n",
    "        return False if self.value == 0 else True\n",
    "    \n",
    "    @classmethod\n",
    "    def from_int_list(cls, int_list):\n",
    "        \"\"\"\n",
    "        Converts a list of integers to a list of Grade\n",
    "        \"\"\"\n",
    "        members = dict((member.value, member) for member in cls.__members__.values())\n",
    "        return [members[value] for value in int_list]\n",
    "\n",
    "    @classmethod\n",
    "    def max_grade(cls):\n",
    "        return max(member.value for member in cls.__members__.values())\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "\n",
    "P: {N N N N N}\n",
    "E: {N N N N R}\n",
    "\n",
    "â€¦\n",
    "P: {HR HR HR HR R}\n",
    "E: {HR HR HR HR HR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_permutations(grade, sequence_length):\n",
    "    \"\"\"\n",
    "    For a given grade and a sequence length n, returns all possible permutations of length n that can be formed. \n",
    "    Note that a generator is returned and not a list\n",
    "    \"\"\"\n",
    "    for i, perm in enumerate(itertools.product(range(len(grade.__members__)), repeat=sequence_length)):\n",
    "        yield grade.from_int_list(perm)\n",
    "\n",
    "def get_all_permutation_pairs(grade, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates all possible pairs of permutations given a grade and a sequence length.\n",
    "    Note that a generator is returned and not a list\n",
    "    \"\"\"\n",
    "    sequence_1 = get_all_permutations(grade, sequence_length)\n",
    "    sequence_2 = get_all_permutations(grade, sequence_length)    \n",
    "    for pair in itertools.product(sequence_1, sequence_2):\n",
    "        yield pair\n",
    "        \n",
    "def count_sequence(seq):\n",
    "    \"\"\"\n",
    "    `len` for generators\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for _ in seq:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# A sanity check to ensure the correct number of sequences are being generated\n",
    "# the `-(3**5)` removes exact duplicates\n",
    "assert count_sequence(get_all_permutation_pairs(Grade, 5)) == (3**5) * (3**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO add assumption that we weed out exactly similar documents (or do WE?) dun dun dun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutations = np.array(list(get_all_permutation_pairs(Grade, 5)), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[N, N, N, N, N],\n",
       "        [N, N, N, N, N]],\n",
       "\n",
       "       [[N, N, N, N, N],\n",
       "        [N, N, N, N, R]],\n",
       "\n",
       "       [[N, N, N, N, N],\n",
       "        [N, N, N, N, HR]],\n",
       "\n",
       "       ..., \n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, N]],\n",
       "\n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, R]],\n",
       "\n",
       "       [[HR, HR, HR, HR, HR],\n",
       "        [HR, HR, HR, HR, HR]]], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[N, N, N, N, N],\n",
       "       [N, N, N, N, N]], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[HR, HR, HR, HR, HR],\n",
       "       [HR, HR, HR, HR, HR]], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "production = np.array(list(map(operator.itemgetter(0), permutations)))\n",
    "experiment = np.array(list(map(operator.itemgetter(1), permutations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain this in detail. It's one of the central assumptions we make. \n",
    "Single Query Assumption.\n",
    "We can assume that the entire experiment is for a single query, so there are only 10 R + 10 HR documents i.e a total of 20 relevant documents.\n",
    "\n",
    "Comment by Ece: Isn't this already given in the description along with the idea that the returned documents will be unique? Our assumption was more in the line of supposing that there are 10 irrelevant documents in the collection, given query q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO double check this\n",
    "relevant_count_production = 20\n",
    "relevant_count_experiment = 20\n",
    "(relevant_count_experiment, relevant_count_production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Evaluation Measures (10 points)\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    0.    0.   ...,  0.25  0.25  0.25]\n"
     ]
    }
   ],
   "source": [
    "class Metric:\n",
    "    \n",
    "    \"\"\"\n",
    "    Common parent for all metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def name(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        \"\"\" TODO \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Precision(Metric):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"precision\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(results))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        # binarize, compute how many are relevant and divide by k\n",
    "        return sum(r.is_relevant for r in results) / k\n",
    "    \n",
    "class Recall(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"recall\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"relevant_count\" in kwargs\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(production))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        # binarize and count, divide by number of relevant documents\n",
    "        # the number of relevant documents is a function of the total number of relevant documents\n",
    "        # and how many *can* be present in a list of length k\n",
    "        # if there are only k = 5 and total = 20, recall can never be 1 even when the all documents are relevant\n",
    "        # therefore, the denom should be min(k, total)\n",
    "        # TODO verify\n",
    "        return sum(r.is_relevant for r in results) / kwargs[\"relevant_count\"] \n",
    "        \n",
    "    \n",
    "class AveragePrecision(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"relevant_count\" in kwargs\n",
    "        P = Precision()\n",
    "        precisions = np.zeros(len(results))\n",
    "        for k in range(1, len(results) + 1):\n",
    "            precisions[k - 1] = P.compute(results, k = k)\n",
    "        # TODO verify\n",
    "        return precisions.sum() / kwargs[\"relevant_count\"] \n",
    "\n",
    "AP = AveragePrecision()\n",
    "ap = np.array([AP.compute(r, relevant_count = relevant_count_production) for r in production])\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Explain why the max in our case can only be 25, so this shit is right!\n",
    "\n",
    "Comment by Ece: I think the calculation of average precision only considers precision values at ranks where relevant documents are returned, does the method above take it into consideration? I might be missing something, but it seems to me that it adds up all the precision values. Also, I didn't quite understand why we divide by relevant_count_production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1215.,   2430.,   4374.,   2916.,   4374.,   8748.,   2916.,\n",
       "          8748.,   7776.,  15552.]),\n",
       " array([ 0.   ,  0.025,  0.05 ,  0.075,  0.1  ,  0.125,  0.15 ,  0.175,\n",
       "         0.2  ,  0.225,  0.25 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngySp4/8DQAesn3/tHDQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fca2a4e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HR, HR, HR, HR, HR, HR, HR, HR, HR, HR, R, R, R, R, R, R, R, R, R, R, N, N, N, N, N, N, N, N, N, N]\n"
     ]
    }
   ],
   "source": [
    "class DCG(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    def compute(self, results, **kwargs):\n",
    "        \n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(production))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        \n",
    "        dcg = 0.0 \n",
    "        for rank, result in enumerate(results):\n",
    "            rank += 1 # TODO verify\n",
    "            gain = 2**result.value - 1\n",
    "            discount = math.log2(1 + rank)\n",
    "            dcg += gain / discount\n",
    "        return dcg\n",
    "\n",
    "\n",
    "class NDCG(Metric):\n",
    "    \"\"\" TODO \"\"\"\n",
    "\n",
    "    # NDCG: We considered two possibilities. TODO explain it\n",
    "    # Normalization is done according to the slides mentioned here \n",
    "    # https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"ideal_ranking\" in kwargs\n",
    "        \n",
    "        dcg = DCG()\n",
    "        \n",
    "        ideal_ranking = kwargs[\"ideal_ranking\"]\n",
    "        # if k is not specified, the entire list is used\n",
    "        k = kwargs.get(\"k\", len(results))\n",
    "        \n",
    "        # only up to k\n",
    "        results = results[:k]\n",
    "        ideal_ranking = ideal_ranking[:k]\n",
    "        \n",
    "        max_dcg = dcg.compute(ideal_ranking)\n",
    "        \n",
    "        return NDCG.dcg.compute(results) / max_dcg\n",
    "\n",
    "ideal_ranking = [Grade.HR] * 10 + [Grade.R] * 10 + [Grade.N] * 10\n",
    "print(ideal_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623046875\n",
      "0.1875\n"
     ]
    }
   ],
   "source": [
    "class ERR(Metric):\n",
    "    # TODO cite this http://delivery.acm.org/10.1145/1650000/1646033/p621-chapelle.pdf?ip=145.109.35.88&id=1646033&acc=ACTIVE%20SERVICE&key=0C390721DC3021FF%2E86041C471C98F6DA%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=1026312371&CFTOKEN=18042814&__acm__=1515748872_543fbe0fe6a5c9d1fadb8f8341c9da3c\n",
    "    # TODO and explain how you converted to code\n",
    "    def compute(self, results, **kwargs):\n",
    "        assert \"grade\" in kwargs\n",
    "        grade = kwargs[\"grade\"]\n",
    "        \n",
    "        def r_func(result):\n",
    "            return ((2 ** result.value) - 1) / (2 ** grade.max_grade())  \n",
    "        \n",
    "        err = 0.0\n",
    "        for rank, result in enumerate(results):\n",
    "            previous_results = results[:rank]\n",
    "            satisfaction = 1\n",
    "            for previous_result in previous_results:\n",
    "                satisfaction = satisfaction * ((1 - r_func(previous_result)))\n",
    "            # rank is 0-indexed here, so add 1\n",
    "            # I am not sure, but I think r_func(result) should be here. Please check TODO\n",
    "            err += (1/(rank + 1)) * satisfaction * r_func(result)\n",
    "        return err\n",
    "# TODO this is incorrect. I couldn't figure out what is going wrong\n",
    "\n",
    "# Comment by Ece: The part where you multiply with r_func(result) seems correct, when compared to slides by Ilya.\n",
    "# I failed to see how this is incorrect, could you explain why?\n",
    "# https://lingpipe-blog.com/2010/03/09/chapelle-metzler-zhang-grinspan-2009-expected-reciprocal-rank-for-graded-relevance/\n",
    "# http://don-metzler.net/presentations/err_cikm09.pdf\n",
    "\n",
    "err = ERR()\n",
    "print(err.compute([Grade.HR, Grade.HR, Grade.HR, Grade.HR], grade=Grade))\n",
    "print(err.compute([Grade.N, Grade.N, Grade.N, Grade.HR], grade=Grade))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the ð›¥measure (0 points)\n",
    "\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ð›¥measure = measureE-measureP. Consider only those pairs for which E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27962"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision = AveragePrecision()\n",
    "ndcg = NDCG() # TODO or pick ERR?\n",
    "\n",
    "# TODO which measure do we use to subset? All 3 or just one? right now I\"m doing only one\n",
    "\n",
    "average_precision_P = np.array([average_precision.compute(results, relevant_count = relevant_count_production) \\\n",
    "                                for results in production])\n",
    "average_precision_E = np.array([average_precision.compute(results, relevant_count = relevant_count_experiment) \\\n",
    "                                for results in experiment])\n",
    "delta_measure = average_precision_E - average_precision_P\n",
    "len(permutations[delta_measure > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutations = permutations[delta_measure > 0]\n",
    "production = production[delta_measure > 0]\n",
    "experiment = experiment[delta_measure > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Interleaving (15 points)\n",
    "Implement 2 interleaving algorithms: \n",
    "(1) Team-Draft Interleaving OR Balanced Interleaving, ~~AND (2), Probabilistic Interleaving.~~ \n",
    "\n",
    "The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toss_coin(heads = 0.5):\n",
    "    # head = True, Tails is False\n",
    "    return True if random.random() < heads else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain\n",
    "\n",
    "Citations for both algorithms: \n",
    "\n",
    "https://www.cs.cornell.edu/people/tj/publications/radlinski_etal_08b.pdf\n",
    "\n",
    "https://www.cs.cornell.edu/people/tj/publications/chapelle_etal_12a.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interleaved(source='A', source_index=0, result=R),\n",
       " Interleaved(source='B', source_index=0, result=R),\n",
       " Interleaved(source='A', source_index=1, result=HR),\n",
       " Interleaved(source='B', source_index=1, result=N),\n",
       " Interleaved(source='A', source_index=2, result=N),\n",
       " Interleaved(source='B', source_index=2, result=HR)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Interleaved = collections.namedtuple(\"Interleaved\", [\"source\", \"source_index\", \"result\"])\n",
    "        \n",
    "class BalancedInterleaving:\n",
    "    \n",
    "    @staticmethod\n",
    "    def interleave(A, B, A_name, B_name):\n",
    "\n",
    "        selected = toss_coin()\n",
    "        interleaved = []\n",
    "        \n",
    "        ka = 0\n",
    "        kb = 0\n",
    "        \n",
    "        while(ka <= len(A) and kb <= len(B) and (ka < len(A) or kb < len(B))):\n",
    "\n",
    "            if (ka < kb) or (ka==kb and selected):\n",
    "                    tup = Interleaved(A_name, ka, A[ka])\n",
    "                    # assumption: all docs are unique, most recently picked document is not in the interleaved list\n",
    "                    interleaved.append(tup)\n",
    "                    ka += 1\n",
    "\n",
    "            else:  \n",
    "                    tup = Interleaved(B_name, kb, B[kb])\n",
    "                    # assumption: all docs are unique, most recently picked document is not in the interleaved list\n",
    "                    interleaved.append(tup)\n",
    "                    kb += 1\n",
    "        \n",
    "        return interleaved\n",
    "    \n",
    "    @staticmethod\n",
    "    def score_clicks(interleaved, clicks):\n",
    "        scores = collections.defaultdict(int)\n",
    "        \n",
    "        #assumption: all docs are unique, just count the clicks belonging to each algorithm\n",
    "\n",
    "        for c in clicks:\n",
    "            il = interleaved[c]\n",
    "            scores[il.source] += 1\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "A = [Grade.R, Grade.HR, Grade.N]\n",
    "B = [Grade.R, Grade.N, Grade.HR]\n",
    "\n",
    "interleaved = BalancedInterleaving.interleave(A, B, \"A\", \"B\")\n",
    "interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'A': 3})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks = [0, 2, 4]\n",
    "BalancedInterleaving.score_clicks(interleaved, clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interleaved(source='A', source_index=0, result=R),\n",
       " Interleaved(source='B', source_index=0, result=R),\n",
       " Interleaved(source='A', source_index=1, result=HR),\n",
       " Interleaved(source='A', source_index=2, result=N),\n",
       " Interleaved(source='B', source_index=1, result=N),\n",
       " Interleaved(source='B', source_index=2, result=HR)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TeamDraftInterleaving:\n",
    "    \"\"\" TODO \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def interleave(A, B, A_name, B_name):\n",
    "\n",
    "        interleaved = []\n",
    "        \n",
    "        ka = 0\n",
    "        kb = 0\n",
    "        \n",
    "        \n",
    "        while ka < len(A) and kb < len(B):\n",
    "            selected = toss_coin()\n",
    "            if selected:\n",
    "                tup = Interleaved(A_name, ka, A[ka])\n",
    "                interleaved.append(tup)\n",
    "                ka += 1\n",
    "            else:\n",
    "                tup = Interleaved(B_name, kb, B[kb])\n",
    "                interleaved.append(tup)\n",
    "                kb += 1\n",
    "        \n",
    "        # add the remaining results\n",
    "        if ka < len(A):\n",
    "            while ka < len(A):\n",
    "                tup = Interleaved(A_name, ka, A[ka])\n",
    "                interleaved.append(tup)\n",
    "                ka += 1\n",
    "        elif kb < len(B):\n",
    "            while kb < len(B):\n",
    "                tup = Interleaved(B_name, kb, B[kb])\n",
    "                interleaved.append(tup)\n",
    "                kb += 1\n",
    "        \n",
    "        return interleaved\n",
    "    \n",
    "    @staticmethod\n",
    "    def score_clicks(interleaved, clicks):\n",
    "        scores = collections.defaultdict(int)\n",
    "        \n",
    "        for c in clicks:\n",
    "            il = interleaved[c]\n",
    "            scores[il.source] += 1\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "A = [Grade.R, Grade.HR, Grade.N]\n",
    "B = [Grade.R, Grade.N, Grade.HR]\n",
    "\n",
    "interleaved = TeamDraftInterleaving.interleave(A, B, \"A\", \"B\")\n",
    "interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sessions:  11717\n",
      "Total number of interactions:  100000\n"
     ]
    }
   ],
   "source": [
    "ClickAction = collections.namedtuple(\"ClickAction\", [\"clicked_id\"])\n",
    "QueryAction = collections.namedtuple(\"QueryAction\", [\"query_id\", \"results\"])\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, session_id):\n",
    "        self.session_id = session_id\n",
    "        self.data = []\n",
    "    \n",
    "    @property\n",
    "    def clicks(self):\n",
    "        return list(filter(lambda _: isinstance(_, ClickAction), self.data))\n",
    "    \n",
    "    @property\n",
    "    def queries(self):\n",
    "        return list(filter(lambda _: isinstance(_, QueryAction), self.data))\n",
    "    \n",
    "    def get_complete_interactions(self):\n",
    "        # return complete interactions\n",
    "        last_query = 0\n",
    "        for index, action in enumerate(self.data):\n",
    "            if index != 0 and isinstance(action, QueryAction):\n",
    "                yield self.data[last_query: index]\n",
    "                last_query = index\n",
    "        if isinstance(self.data[-1], ClickAction):\n",
    "            yield self.data[last_query:]\n",
    "        \n",
    "class YandexData:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.sessions = []\n",
    "        self.u = 0\n",
    "        self.q = 0\n",
    "        self._read()\n",
    "        \n",
    "    def _read(self):\n",
    "        with codecs.open(self.file_path, \"r\", \"utf-8\") as reader:\n",
    "            current_session = Session(0)\n",
    "            for line in reader:\n",
    "                line = line.split(\"\\t\")\n",
    "                session_id = int(line[0])\n",
    "                interaction_type = line[2]\n",
    "                if current_session.session_id != session_id:\n",
    "                    self.sessions.append(current_session)\n",
    "                    current_session = Session(session_id)\n",
    "                \n",
    "                if interaction_type == \"C\":\n",
    "                    current_session.data.append(ClickAction(int(line[3])))\n",
    "                else:\n",
    "                    results = [int(q_id) for q_id in line[5:]]\n",
    "                    self.u = max(self.u, max(results))\n",
    "                    query_id = int(line[3])\n",
    "                    self.q = max(self.q, query_id)\n",
    "                    current_session.data.append(QueryAction(query_id, results))\n",
    "        \n",
    "        self.sessions.append(current_session)\n",
    "\n",
    "\n",
    "yandex_data = YandexData(\"./YandexRelPredChallenge.txt\")\n",
    "print(\"Total number of sessions: \", len(yandex_data.sessions))\n",
    "print(\"Total number of interactions: \", sum(len(session.data) for session in yandex_data.sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13445559411047547"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomClickModel:\n",
    "    def __init__(self):\n",
    "        self.rho = 0\n",
    "    \n",
    "    def estimate(self, yandex_data):\n",
    "        clicks = 0\n",
    "        results = 0\n",
    "        for session in yandex_data.sessions:\n",
    "            clicks += len(session.clicks)\n",
    "            results += sum(len(query.results) for query in session.queries)\n",
    "        self.rho = clicks / results\n",
    "    \n",
    "    def predict(self, clicks):\n",
    "        #(b) there is a method that predicts the click probability given a ranked list of relevance labels,\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def simulate(results, grade_to_probability, params):\n",
    "        rho = params.get(\"rho\")\n",
    "        assert 0 <= rho <= 1.0\n",
    "        \n",
    "        # TODO something is wrong with this and I can't figure it out. pls fix \n",
    "        clicks = []\n",
    "        for rank, result in enumerate(results):\n",
    "            click = toss_coin(rho)\n",
    "            if click:\n",
    "                clicks.append(rank)\n",
    "        return clicks\n",
    "\n",
    "rcm = RandomClickModel()\n",
    "rcm.estimate(yandex_data)\n",
    "rcm.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527494, 66579)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yandex_data.u, yandex_data.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDynamicBayesianNetwork:\n",
    "        \n",
    "    def _estimate_sigma(self, yandex_data):\n",
    "        satisfied_clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        total_clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        \n",
    "        for session in yandex_data.sessions:\n",
    "            for interaction in session.get_complete_interactions():\n",
    "                #print(interaction) TODO explain why we continue\n",
    "                if not isinstance(interaction[-1], ClickAction):\n",
    "                    continue\n",
    "                query_action = interaction[0]\n",
    "                # sanity check TODO remove\n",
    "                assert isinstance(query_action, QueryAction)\n",
    "                for click in interaction[1:]:\n",
    "                    # sanity check TODO remove\n",
    "                    assert isinstance(click, ClickAction)\n",
    "                    total_clicks[click.clicked_id][query_action.query_id] += 1\n",
    "                \n",
    "                last_click = interaction[-1]\n",
    "                #print(last_click)\n",
    "                satisfied_clicks[last_click.clicked_id][query_action.query_id] += 1\n",
    "        \n",
    "        self.sigma = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        all_clicks = 0\n",
    "        all_satisfied = 0\n",
    "        for click_id, queries in total_clicks.items():\n",
    "            for query_id, count in queries.items():\n",
    "                self.sigma[click_id][query_id] = satisfied_clicks[click_id][query_id] / count\n",
    "                all_clicks += count\n",
    "                all_satisfied += satisfied_clicks[click_id][query_id]\n",
    "        \n",
    "        self.sigma_global = all_satisfied / all_clicks\n",
    "                \n",
    "    def _estimate_alpha(self, yandex_data):\n",
    "        seen = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        clicks = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        \n",
    "        for session in yandex_data.sessions:\n",
    "            for interaction in session.get_complete_interactions():\n",
    "                query_action = interaction[0]\n",
    "                assert isinstance(query_action, QueryAction)\n",
    "                \n",
    "                for result in query_action.results:\n",
    "                    seen[result][query_action.query_id] += 1\n",
    "                \n",
    "                for click in interaction[1:]:\n",
    "                    # sanity check TODO remove\n",
    "                    assert isinstance(click, ClickAction)\n",
    "                    clicks[click.clicked_id][query_action.query_id] += 1\n",
    "                \n",
    "\n",
    "        \n",
    "        self.alpha = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "        for click_id, queries in seen.items():\n",
    "            for query_id, count in queries.items():\n",
    "                self.alpha[click_id][query_id] = clicks[click_id][query_id] / count\n",
    "    \n",
    "    \n",
    "    def estimate(self, yandex_data):\n",
    "        self._estimate_sigma(yandex_data)\n",
    "        self._estimate_alpha(yandex_data)\n",
    "        \n",
    "    @staticmethod\n",
    "    def simulate(results, grade_to_probability, params):\n",
    "        clicks = []\n",
    "        for index, result in enumerate(results):\n",
    "            examine = toss_coin(grade_to_probability[result])\n",
    "            if not examine:\n",
    "                continue\n",
    "            clicks.append(index)\n",
    "            \n",
    "            satisfied = toss_coin(grade_to_probability[result])\n",
    "            if satisfied:\n",
    "                break\n",
    "                \n",
    "        return clicks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5121189928157913"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TODO compute one sigma from all of it\n",
    "    \n",
    "# TODO replace with how you compute alpha\n",
    "grade_to_probability = {\n",
    "    Grade.N: 0.1,\n",
    "    Grade.R: 0.5,\n",
    "    Grade.HR: 0.9\n",
    "}\n",
    "    \n",
    "sdbn = SimpleDynamicBayesianNetwork()\n",
    "sdbn.estimate(yandex_data)\n",
    "sdbn.sigma_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(interleaving, click_model, params):\n",
    "    winner_E = 0\n",
    "    for production, experimental in permutations:\n",
    "        interleaved = interleaving.interleave(production, experimental, \"P\", \"E\")\n",
    "        clicks = click_model.simulate([i.result for i in interleaved], grade_to_probability, params)\n",
    "        scores = interleaving.score_clicks(interleaved, clicks)\n",
    "        if scores[\"E\"] > scores[\"P\"]:\n",
    "            winner_E += 1\n",
    "    return (winner_E / len(permutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27962"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO permutations Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3019454974608397"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, RandomClickModel, {\"rho\": rcm.rho})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5642657892854588"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30412702954009013"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(BalancedInterleaving, RandomClickModel, {\"rho\": rcm.rho})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5629068020885487"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(TeamDraftInterleaving, SimpleDynamicBayesianNetwork, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part\n",
    "\n",
    "## 1.a\n",
    "P($m^{th}$ experiment gives significant result | m experiments lacking power to reject H0) = $(1-\\alpha)^{m-1}\\alpha$\n",
    "\n",
    "## 1.b\n",
    "P(at least one significant result | m experiments lacking power to reject H0) = 1 - $(1-\\alpha)^m$\n",
    "\n",
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
